{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d202871b-0906-4279-83dc-61837f6ea2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, EngineArgs, PoolingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641341dd-21d3-4825-85d3-f774961ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Follow the white rabbit.\",  # English\n",
    "    \"Sigue al conejo blanco.\",  # Spanish\n",
    "    \"Suis le lapin blanc.\",  # French\n",
    "    \"跟着白兔走。\",  # Chinese\n",
    "    \"اتبع الأرنب الأبيض.\",  # Arabic\n",
    "    \"Folge dem weißen Kaninchen.\",  # German\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843e9fc0-84f5-46d1-98af-b8dac2c49bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9306f93d38894dd98f3fff6e105a3e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12e4422e44e45449034fd9b3be90f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_xlm_roberta.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
      "- configuration_xlm_roberta.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b03d95a0864ea4ad8b74315b77e74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/378 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-24 19:56:49 [config.py:484] Found sentence-transformers modules configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5cf2d293d640499c15d9b5cfc3b833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-24 19:56:49 [config.py:504] Found pooling configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cd02e8b76c43408cb9a2685d7a1315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-24 19:56:49 [config.py:1467] Using max model len 8194\n",
      "WARNING 09-24 19:56:49 [arg_utils.py:1719] ['XLMRobertaModel'] is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 09-24 19:56:56 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "INFO 09-24 19:56:56 [config.py:4581] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 09-24 19:56:56 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='jinaai/jina-embeddings-v3', speculative_config=None, tokenizer='jinaai/jina-embeddings-v3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8194, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=jinaai/jina-embeddings-v3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='MEAN', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24659b254ffc49cf8e3b1de6e6ef2258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65334f3e201c48ceb210190b57017ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-24 19:56:59 [rocm.py:233] Using ROCmFlashAttention backend.\n",
      "INFO 09-24 19:56:59 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-24 19:56:59 [model_runner.py:1171] Starting to load model jinaai/jina-embeddings-v3...\n",
      "INFO 09-24 19:57:00 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4653168b39469ab4aec02b74bc22c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-24 19:57:02 [weight_utils.py:308] Time spent downloading weights for jinaai/jina-embeddings-v3: 2.445407 seconds\n",
      "INFO 09-24 19:57:02 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fef2f3b08b241b3acf6a92f329939ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-24 19:57:10 [default_loader.py:272] Loading weights took 7.89 seconds\n",
      "INFO 09-24 19:57:10 [model_runner.py:1203] Model loading took 4.1230 GiB and 10.823411 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\n",
    "    model=\"jinaai/jina-embeddings-v3\",\n",
    "    task=\"embed\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd9bda-7099-4ae1-ad57-f4e298f76b6e",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2810ff32-2be3-4e9f-aace-603d067988c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b423a29efe406786a6e0bad6d4ee06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2660b19764c4ef4a6e42be9ed2c32d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = model.embed(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9209a35-4312-4714-993e-18ab97117a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Outputs:\n",
      "------------------------------------------------------------\n",
      "Prompt: 'Follow the white rabbit.' \n",
      "Embeddings for text matching: [-0.14213059842586517, -0.05029097571969032, 0.01353879552334547, 0.04582967981696129, 0.08003295212984085, 0.03540872409939766, -0.0008887738804332912, 0.05841369181871414, -0.04777868092060089, -0.03277672827243805, -0.07327341288328171, 0.061782196164131165, -0.08115953952074051, 0.06381005793809891, -0.08413373678922653, -0.026829740032553673, ...] (size=1024)\n",
      "------------------------------------------------------------\n",
      "Prompt: 'Sigue al conejo blanco.' \n",
      "Embeddings for text matching: [-0.05010094866156578, -0.047637730836868286, -0.045011844485998154, -0.025765040889382362, 0.13794027268886566, -0.027189088985323906, -0.002701407764106989, 0.052564166486263275, -0.08811818063259125, 0.006689615547657013, -0.02919844165444374, 0.001295513822697103, -0.038563322275877, 0.08588734269142151, -0.12074421346187592, 0.06291375309228897, ...] (size=1024)\n",
      "------------------------------------------------------------\n",
      "Prompt: 'Suis le lapin blanc.' \n",
      "Embeddings for text matching: [-0.12992610037326813, -0.04526917263865471, -0.08124836534261703, 0.016199685633182526, 0.07454264909029007, 0.0103927506133914, 0.0547151044011116, 0.03038041666150093, -0.0892016589641571, 0.04353148117661476, 0.04411071166396141, 0.08153797686100006, -0.13037165999412537, -0.01795060932636261, -0.09401373565196991, -0.01097964122891426, ...] (size=1024)\n",
      "------------------------------------------------------------\n",
      "Prompt: '跟着白兔走。' \n",
      "Embeddings for text matching: [-0.019831771031022072, -0.1456122100353241, 0.012421311810612679, 0.019188253208994865, 0.13938868045806885, -0.04923907667398453, -0.105342335999012, 0.08552772551774979, -0.0819583535194397, 0.017257701605558395, -0.02975623495876789, 0.1338973343372345, -0.09106483310461044, -0.046447642147541046, -0.06198357790708542, 0.03271927312016487, ...] (size=1024)\n",
      "------------------------------------------------------------\n",
      "Prompt: 'اتبع الأرنب الأبيض.' \n",
      "Embeddings for text matching: [-0.09543018043041229, -0.046100303530693054, -0.05541263893246651, 0.02095882222056389, 0.06993356347084045, -0.008796331472694874, 0.04565107822418213, 0.03866075724363327, 0.02020910009741783, 0.035033561289310455, -0.008844517171382904, 0.007797713857144117, -0.10349196195602417, 0.045966748148202896, -0.11801289021968842, 0.02120164781808853, ...] (size=1024)\n",
      "------------------------------------------------------------\n",
      "Prompt: 'Folge dem weißen Kaninchen.' \n",
      "Embeddings for text matching: [-0.07208915799856186, -0.06615692377090454, -0.008017639629542828, -0.00028347308398224413, 0.1465204954147339, 0.007201716769486666, 0.0007575781201012433, 0.07214675098657608, -0.14102980494499207, 0.014880986884236336, -0.055722709745168686, 0.05590509623289108, -0.06231728568673134, 0.08819642663002014, -0.10136637836694717, 0.00974547490477562, ...] (size=1024)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerated Outputs:\")\n",
    "print(\"-\" * 60)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    embeds = output.outputs.embedding\n",
    "    embeds_trimmed = (\n",
    "        (str(embeds[:16])[:-1] + \", ...]\") if len(embeds) > 16 else embeds\n",
    "    )\n",
    "    print(\n",
    "        f\"Prompt: {prompt!r} \\n\"\n",
    "        f\"Embeddings for text matching: {embeds_trimmed} \"\n",
    "        f\"(size={len(embeds)})\"\n",
    "    )\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e0c23-7d7b-4efe-9c8a-7f569f2f192f",
   "metadata": {},
   "source": [
    "#### With PoolingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6274fc9-c857-46a2-9493-64c37b9ac20b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e1d22ee3444d41a1ec3d5db5013fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0304ae43bd73420b928a1bbb9e28f9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = model.embed(prompts, pooling_params=PoolingParams(dimensions=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cbf5d22-81d1-4fb1-ac61-a02090b1ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Outputs:\n",
      "------------------------------------------------------------\n",
      "Prompt: 'Follow the white rabbit.' \n",
      "Embeddings: [-0.38497915863990784, -0.13621963560581207, 0.036671582609415054, 0.12413563579320908, 0.21677963435649872, 0.09590911865234375, -0.0024073594249784946, 0.1582210659980774, -0.12941475212574005, -0.08878002315759659, -0.19847054779529572, 0.1673450917005539, -0.21983115375041962, 0.17283782362937927, -0.22788715362548828, -0.07267183065414429, ...] (size=32)\n",
      "------------------------------------------------------------\n",
      "Prompt: 'Sigue al conejo blanco.' \n",
      "Embeddings: [-0.13791604340076447, -0.13113538920879364, -0.1239069476723671, -0.07092504948377609, 0.3797169029712677, -0.07484512031078339, -0.007436335552483797, 0.1446966975927353, -0.2425684779882431, 0.018414925783872604, -0.08037639409303665, 0.003566242754459381, -0.10615569353103638, 0.23642750084400177, -0.3323802053928375, 0.17318665981292725, ...] (size=32)\n",
      "------------------------------------------------------------\n",
      "Prompt: 'Suis le lapin blanc.' \n",
      "Embeddings: [-0.3880743086338043, -0.13521380722522736, -0.24267952144145966, 0.048386599868535995, 0.22265030443668365, 0.031041951850056648, 0.16342772543430328, 0.0907428115606308, -0.26643508672714233, 0.1300235241651535, 0.13175362348556519, 0.2435445636510849, -0.38940516114234924, -0.05361640453338623, -0.2808082103729248, -0.03279492259025574, ...] (size=32)\n",
      "------------------------------------------------------------\n",
      "Prompt: '跟着白兔走。' \n",
      "Embeddings: [-0.04960673674941063, -0.36423105001449585, 0.03107038512825966, 0.04799705743789673, 0.34866365790367126, -0.12316549569368362, -0.2635008990764618, 0.21393708884716034, -0.20500874519348145, 0.043168019503355026, -0.07443156838417053, 0.3349277377128601, -0.22778749465942383, -0.11618306487798691, -0.1550443023443222, 0.08184324204921722, ...] (size=32)\n",
      "------------------------------------------------------------\n",
      "Prompt: 'اتبع الأرنب الأبيض.' \n",
      "Embeddings: [-0.2893688976764679, -0.13978800177574158, -0.16802538931369781, 0.06355255097150803, 0.21205659210681915, -0.026672743260860443, 0.1384258270263672, 0.11722937226295471, 0.06127920001745224, 0.10623078048229218, -0.026818854734301567, 0.02364467829465866, -0.31381431221961975, 0.13938303291797638, -0.3578454852104187, 0.06428885459899902, ...] (size=32)\n",
      "------------------------------------------------------------\n",
      "Prompt: 'Folge dem weißen Kaninchen.' \n",
      "Embeddings: [-0.18767470121383667, -0.17223089933395386, -0.020872876048088074, -0.0007379851303994656, 0.3814469575881958, 0.018748726695775986, 0.00197225552983582, 0.18782463669776917, -0.3671526610851288, 0.03874070569872856, -0.14506679773330688, 0.14554160833358765, -0.16223490238189697, 0.22960786521434784, -0.26389411091804504, 0.025371070951223373, ...] (size=32)\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerated Outputs:\")\n",
    "print(\"-\" * 60)\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "    embeds = output.outputs.embedding\n",
    "    embeds_trimmed = (\n",
    "        (str(embeds[:16])[:-1] + \", ...]\") if len(embeds) > 16 else embeds\n",
    "    )\n",
    "    print(f\"Prompt: {prompt!r} \\nEmbeddings: {embeds_trimmed} (size={len(embeds)})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a0f11-887c-4897-a03f-128200a44344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
