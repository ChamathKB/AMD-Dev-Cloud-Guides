{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcc02c32-5e79-4a1e-a0be-e9d6a639d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65a8c595-ae86-4765-b001-ca4fca9cde1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "def clean_up(llm):\n",
    "    try:\n",
    "        del llm\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to unload model: {e}\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5570ff-e4d7-496f-99db-28db35cd51d8",
   "metadata": {},
   "source": [
    "### Automatic Prefix Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0444a61e-e294-4174-9904-b1bb454b0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_PROMPT = (\n",
    "    \"You are a helpful assistant in recognizes the content of tables in markdown format. Here is a table as follows.\\n# Table\\n\"\n",
    "    + \"\"\"\n",
    "| ID  | Name          | Age | Occupation    | Country       | Email                  | Phone Number   | Address                       |\n",
    "|-----|---------------|-----|---------------|---------------|------------------------|----------------|------------------------------|\n",
    "| 1   | John Doe      | 29  | Engineer      | USA           | john.doe@example.com   | 555-1234       | 123 Elm St, Springfield, IL  |\n",
    "| 2   | Jane Smith    | 34  | Doctor        | Canada        | jane.smith@example.com | 555-5678       | 456 Oak St, Toronto, ON      |\n",
    "| 3   | Alice Johnson | 27  | Teacher       | UK            | alice.j@example.com    | 555-8765       | 789 Pine St, London, UK      |\n",
    "| 4   | Bob Brown     | 45  | Artist        | Australia     | bob.b@example.com      | 555-4321       | 321 Maple St, Sydney, NSW    |\n",
    "| 5   | Carol White   | 31  | Scientist     | New Zealand   | carol.w@example.com    | 555-6789       | 654 Birch St, Wellington, NZ |\n",
    "| 6   | Dave Green    | 28  | Lawyer        | Ireland       | dave.g@example.com     | 555-3456       | 987 Cedar St, Dublin, IE     |\n",
    "| 7   | Emma Black    | 40  | Musician      | USA           | emma.b@example.com     | 555-1111       | 246 Ash St, New York, NY     |\n",
    "| 8   | Frank Blue    | 37  | Chef          | Canada        | frank.b@example.com    | 555-2222       | 135 Spruce St, Vancouver, BC |\n",
    "| 9   | Grace Yellow  | 50  | Engineer      | UK            | grace.y@example.com    | 555-3333       | 864 Fir St, Manchester, UK   |\n",
    "| 10  | Henry Violet  | 32  | Artist        | Australia     | henry.v@example.com    | 555-4444       | 753 Willow St, Melbourne, VIC|\n",
    "| 11  | Irene Orange  | 26  | Scientist     | New Zealand   | irene.o@example.com    | 555-5555       | 912 Poplar St, Auckland, NZ  |\n",
    "| 12  | Jack Indigo   | 38  | Teacher       | Ireland       | jack.i@example.com     | 555-6666       | 159 Elm St, Cork, IE         |\n",
    "| 13  | Karen Red     | 41  | Lawyer        | USA           | karen.r@example.com    | 555-7777       | 357 Cedar St, Boston, MA     |\n",
    "| 14  | Leo Brown     | 30  | Chef          | Canada        | leo.b@example.com      | 555-8888       | 246 Oak St, Calgary, AB      |\n",
    "| 15  | Mia Green     | 33  | Musician      | UK            | mia.g@example.com      | 555-9999       | 975 Pine St, Edinburgh, UK   |\n",
    "| 16  | Noah Yellow   | 29  | Doctor        | Australia     | noah.y@example.com     | 555-0000       | 864 Birch St, Brisbane, QLD  |\n",
    "| 17  | Olivia Blue   | 35  | Engineer      | New Zealand   | olivia.b@example.com   | 555-1212       | 753 Maple St, Hamilton, NZ   |\n",
    "| 18  | Peter Black   | 42  | Artist        | Ireland       | peter.b@example.com    | 555-3434       | 912 Fir St, Limerick, IE     |\n",
    "| 19  | Quinn White   | 28  | Scientist     | USA           | quinn.w@example.com    | 555-5656       | 159 Willow St, Seattle, WA   |\n",
    "| 20  | Rachel Red    | 31  | Teacher       | Canada        | rachel.r@example.com   | 555-7878       | 357 Poplar St, Ottawa, ON    |\n",
    "| 21  | Steve Green   | 44  | Lawyer        | UK            | steve.g@example.com    | 555-9090       | 753 Elm St, Birmingham, UK   |\n",
    "| 22  | Tina Blue     | 36  | Musician      | Australia     | tina.b@example.com     | 555-1213       | 864 Cedar St, Perth, WA      |\n",
    "| 23  | Umar Black    | 39  | Chef          | New Zealand   | umar.b@example.com     | 555-3435       | 975 Spruce St, Christchurch, NZ|\n",
    "| 24  | Victor Yellow | 43  | Engineer      | Ireland       | victor.y@example.com   | 555-5657       | 246 Willow St, Galway, IE    |\n",
    "| 25  | Wendy Orange  | 27  | Artist        | USA           | wendy.o@example.com    | 555-7879       | 135 Elm St, Denver, CO       |\n",
    "| 26  | Xavier Green  | 34  | Scientist     | Canada        | xavier.g@example.com   | 555-9091       | 357 Oak St, Montreal, QC     |\n",
    "| 27  | Yara Red      | 41  | Teacher       | UK            | yara.r@example.com     | 555-1214       | 975 Pine St, Leeds, UK       |\n",
    "| 28  | Zack Blue     | 30  | Lawyer        | Australia     | zack.b@example.com     | 555-3436       | 135 Birch St, Adelaide, SA   |\n",
    "| 29  | Amy White     | 33  | Musician      | New Zealand   | amy.w@example.com      | 555-5658       | 159 Maple St, Wellington, NZ |\n",
    "| 30  | Ben Black     | 38  | Chef          | Ireland       | ben.b@example.com      | 555-7870       | 246 Fir St, Waterford, IE    |\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "deeb1205-e7fa-47e9-b5b3-f38ce22601eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generation_time(llm, sampling_params, prompts):\n",
    "\n",
    "    start_time = time.time()\n",
    "    output = llm.generate(prompts, sampling_params=sampling_params)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Output: {output[0].outputs[0].text}\")\n",
    "    print(f\"Generation time: {end_time - start_time} seconds.\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0dd7b8b1-fa57-4539-9055-1a1eab06dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:05:03 [config.py:853] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 09-05 20:05:03 [config.py:1467] Using max model len 16384\n",
      "INFO 09-05 20:05:03 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-05 20:05:03 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "INFO 09-05 20:05:06 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-05 20:05:15 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-05 20:05:15 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='lmsys/longchat-13b-16k', speculative_config=None, tokenizer='lmsys/longchat-13b-16k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=lmsys/longchat-13b-16k, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-05 20:05:15 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4432c14500>\n",
      "INFO 09-05 20:05:15 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-05 20:05:16 [gpu_model_runner.py:1751] Starting to load model lmsys/longchat-13b-16k...\n",
      "INFO 09-05 20:05:16 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-05 20:05:16 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-05 20:05:16 [weight_utils.py:292] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading pt checkpoint shards:  33% Completed | 1/3 [00:05<00:10,  5.03s/it]\n",
      "Loading pt checkpoint shards:  67% Completed | 2/3 [00:08<00:03,  3.89s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 3/3 [00:13<00:00,  4.38s/it]\n",
      "Loading pt checkpoint shards: 100% Completed | 3/3 [00:13<00:00,  4.36s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:05:29 [default_loader.py:272] Loading weights took 13.09 seconds\n",
      "INFO 09-05 20:05:29 [gpu_model_runner.py:1782] Model loading took 24.3262 GiB and 13.505399 seconds\n",
      "INFO 09-05 20:05:35 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/a3ae9284dc/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-05 20:05:35 [backends.py:520] Dynamo bytecode transform time: 5.73 s\n",
      "INFO 09-05 20:05:38 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 0.368 s\n",
      "INFO 09-05 20:05:39 [monitor.py:34] torch.compile takes 5.73 s in total\n",
      "INFO 09-05 20:05:52 [gpu_worker.py:232] Available KV cache memory: 144.97 GiB\n",
      "INFO 09-05 20:05:52 [kv_cache_utils.py:716] GPU KV cache size: 190,000 tokens\n",
      "INFO 09-05 20:05:52 [kv_cache_utils.py:720] Maximum concurrency for 16,384 tokens per request: 11.60x\n",
      "INFO 09-05 20:05:52 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:09<00:00,  6.90it/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:06:02 [gpu_model_runner.py:2306] Graph capturing finished in 10 secs, took 0.29 GiB\n",
      "INFO 09-05 20:06:02 [core.py:172] init engine (profile, create kv cache, warmup model) took 32.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e6f434de0748878fb8908136845802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e084f1c592a34ac09a6b2271a4ef317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Output: 29.\n",
      "Generation time: 0.32152247428894043 seconds.\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c39a1a198954aaaae7b72ff19f268d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3344ccb6a2642b194a48041f79d7af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Output: 30.\n",
      "Generation time: 0.05277061462402344 seconds.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"lmsys/longchat-13b-16k\", enable_prefix_caching=True)\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=100)\n",
    "\n",
    "# first query\n",
    "get_generation_time(\n",
    "    llm,\n",
    "    sampling_params,\n",
    "    LONG_PROMPT\n",
    "    + \"Question: what is the age of John Doe? Your answer: The age of John Doe is \",\n",
    ")\n",
    "\n",
    "# second query\n",
    "get_generation_time(\n",
    "    llm,\n",
    "    sampling_params,\n",
    "    LONG_PROMPT\n",
    "    + \"Question: what is the age of Zack Blue? Your answer: The age of Zack Blue is \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80b83c52-8dbe-48ba-8cd0-30be33c16937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W905 20:07:30.301253136 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del llm\n",
    "except Exception as e:\n",
    "    print(f\"Failed to unload model: {e}\")\n",
    "finally:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a09ff-59fd-46eb-a7dd-a23c8e4779c6",
   "metadata": {},
   "source": [
    "### Context Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "129b470a-83fd-4185-8998-a985ab8d01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm():\n",
    "    rope_theta = 1000000\n",
    "    original_max_position_embeddings = 32768\n",
    "    factor = 4.0\n",
    "\n",
    "    hf_overrides = {\n",
    "        \"rope_theta\": rope_theta,\n",
    "        \"rope_scaling\": {\n",
    "            \"rope_type\": \"yarn\",\n",
    "            \"factor\": factor,\n",
    "            \"original_max_position_embeddings\": original_max_position_embeddings,\n",
    "        },\n",
    "        \"max_model_len\": int(original_max_position_embeddings * factor),\n",
    "    }\n",
    "\n",
    "    llm = LLM(model=\"Qwen/Qwen3-0.6B\", hf_overrides=hf_overrides)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7450d91b-33e8-48c2-a4b5-c849ee2a30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_chat(llm):\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        max_tokens=128,\n",
    "    )\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"},\n",
    "    ]\n",
    "    outputs = llm.chat(conversation, sampling_params, use_tqdm=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b64d28c2-821b-4bb7-a4e8-7d3928157bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outputs(outputs):\n",
    "    print(\"\\nGenerated Outputs:\\n\" + \"-\" * 80)\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}\\n\")\n",
    "        print(f\"Generated text: {generated_text!r}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "442216cd-3847-425c-93ca-7467a2af697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:07:43 [config.py:853] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 09-05 20:07:43 [config.py:1467] Using max model len 131072\n",
      "INFO 09-05 20:07:43 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-05 20:07:43 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "INFO 09-05 20:07:46 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-05 20:07:55 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-05 20:07:55 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-0.6B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-05 20:07:56 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x71c467561b80>\n",
      "INFO 09-05 20:07:56 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-05 20:07:56 [gpu_model_runner.py:1751] Starting to load model Qwen/Qwen3-0.6B...\n",
      "INFO 09-05 20:07:56 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-05 20:07:56 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-05 20:07:56 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-05 20:07:56 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:07:57 [default_loader.py:272] Loading weights took 0.50 seconds\n",
      "INFO 09-05 20:07:57 [gpu_model_runner.py:1782] Model loading took 1.2324 GiB and 1.000104 seconds\n",
      "INFO 09-05 20:08:02 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/3096bb6dc7/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-05 20:08:02 [backends.py:520] Dynamo bytecode transform time: 4.92 s\n",
      "INFO 09-05 20:08:05 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 0.702 s\n",
      "INFO 09-05 20:08:05 [monitor.py:34] torch.compile takes 4.92 s in total\n",
      "INFO 09-05 20:08:18 [gpu_worker.py:232] Available KV cache memory: 164.94 GiB\n",
      "INFO 09-05 20:08:18 [kv_cache_utils.py:716] GPU KV cache size: 1,544,160 tokens\n",
      "INFO 09-05 20:08:18 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 11.78x\n",
      "INFO 09-05 20:08:18 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:08<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:08:26 [gpu_model_runner.py:2306] Graph capturing finished in 8 secs, took 0.20 GiB\n",
      "INFO 09-05 20:08:26 [core.py:172] init engine (profile, create kv cache, warmup model) took 28.95 seconds\n",
      "\n",
      "Generated Outputs:\n",
      "--------------------------------------------------------------------------------\n",
      "Prompt: None\n",
      "\n",
      "Generated text: '<think>\\nOkay, the user just said \"Hello.\" I need to respond appropriately. Since they\\'re asking hello, my job is to acknowledge it. I should make sure to keep the response friendly and helpful. Maybe add a smiley to keep it warm. Let me check if there\\'s any context I\\'m missing. No, there\\'s nothing given. So I\\'ll go with a simple greeting and offer assistance. Let me make sure the response is clear and matches the tone. Alright, time to send it.\\n'\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "llm = create_llm()\n",
    "outputs = run_llm_chat(llm)\n",
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20c3cd81-ab06-47c9-99a0-d8f69204b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W905 20:11:17.612676525 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del llm\n",
    "except Exception as e:\n",
    "    print(f\"Failed to unload model: {e}\")\n",
    "finally:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86206c46-977a-4749-a7b7-57890c3629ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
