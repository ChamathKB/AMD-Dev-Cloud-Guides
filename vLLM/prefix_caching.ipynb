{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94cc5a0-2240-4362-9537-367b03b69063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.distributed import cleanup_dist_env_and_memory\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb9169f-22da-4072-b175-574de7bcefed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = (\n",
    "    \"You are an expert school principal, skilled in effectively managing \"\n",
    "    \"faculty and staff. Draft 10-15 questions for a potential first grade \"\n",
    "    \"Head Teacher for my K-12, all-girls', independent school that emphasizes \"\n",
    "    \"community, joyful discovery, and life-long learning. The candidate is \"\n",
    "    \"coming in for a first-round panel interview for a 8th grade Math \"\n",
    "    \"teaching role. They have 5 years of previous teaching experience \"\n",
    "    \"as an assistant teacher at a co-ed, public school with experience \"\n",
    "    \"in middle school math teaching. Based on these information, fulfill \"\n",
    "    \"the following paragraph: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac86e431-0ec9-495d-b3f4-f666fab547ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb274734-9d66-40b8-9d0b-d196f04a886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generating_prompts = [prefix + prompt for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd07699-5a61-4e58-8ee8-a73155343b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c5904-78e9-4624-b8a1-3a67463ec661",
   "metadata": {},
   "source": [
    "### Prefix caching disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee94903-5f0b-4eb3-83c5-caaccbeb10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:51:41 [config.py:853] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 09-05 20:51:41 [config.py:1467] Using max model len 2048\n",
      "INFO 09-05 20:51:41 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-05 20:51:41 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "INFO 09-05 20:51:43 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-05 20:51:52 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-05 20:51:53 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-05 20:51:53 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7dc1b8e0ef60>\n",
      "INFO 09-05 20:51:53 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-05 20:51:53 [gpu_model_runner.py:1751] Starting to load model facebook/opt-125m...\n",
      "INFO 09-05 20:51:53 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-05 20:51:53 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-05 20:51:53 [weight_utils.py:292] Using model weights format ['*.bin']\n",
      "INFO 09-05 20:51:53 [default_loader.py:272] Loading weights took 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.65it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.65it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:51:54 [gpu_model_runner.py:1782] Model loading took 0.2500 GiB and 0.358850 seconds\n",
      "INFO 09-05 20:51:55 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/c36373a52c/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-05 20:51:55 [backends.py:520] Dynamo bytecode transform time: 1.37 s\n",
      "INFO 09-05 20:51:57 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 0.808 s\n",
      "INFO 09-05 20:51:57 [monitor.py:34] torch.compile takes 1.37 s in total\n",
      "INFO 09-05 20:52:09 [gpu_worker.py:232] Available KV cache memory: 73.79 GiB\n",
      "INFO 09-05 20:52:10 [kv_cache_utils.py:716] GPU KV cache size: 2,149,280 tokens\n",
      "INFO 09-05 20:52:10 [kv_cache_utils.py:720] Maximum concurrency for 2,048 tokens per request: 1049.45x\n",
      "INFO 09-05 20:52:10 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs:  99%|█████████▊| 66/67 [00:06<00:00, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:52:16 [gpu_model_runner.py:2306] Graph capturing finished in 7 secs, took 0.19 GiB\n",
      "INFO 09-05 20:52:16 [core.py:172] init engine (profile, create kv cache, warmup model) took 22.72 seconds\n",
      "Results without `enable_prefix_caching`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:06<00:00,  9.92it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122deda4fe994149bb2737510b248d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915626bc05f74e7f9ef7f67bafa9d81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Prompt: \"You are an expert school principal, skilled in effectively managing faculty and staff. Draft 10-15 questions for a potential first grade Head Teacher for my K-12, all-girls', independent school that emphasizes community, joyful discovery, and life-long learning. The candidate is coming in for a first-round panel interview for a 8th grade Math teaching role. They have 5 years of previous teaching experience as an assistant teacher at a co-ed, public school with experience in middle school math teaching. Based on these information, fulfill the following paragraph: Hello, my name is\"\n",
      "Generated text: ' Sarah, I am a teacher at a private school in the city of New York'\n",
      "Generation time: 0.028345346450805664 seconds.\n",
      "--------------------------------------------------\n",
      "Prompt: \"You are an expert school principal, skilled in effectively managing faculty and staff. Draft 10-15 questions for a potential first grade Head Teacher for my K-12, all-girls', independent school that emphasizes community, joyful discovery, and life-long learning. The candidate is coming in for a first-round panel interview for a 8th grade Math teaching role. They have 5 years of previous teaching experience as an assistant teacher at a co-ed, public school with experience in middle school math teaching. Based on these information, fulfill the following paragraph: The president of the United States is\"\n",
      "Generated text: ' the head of the United States Department of Education. The president of the United States'\n",
      "Generation time: 0.028345346450805664 seconds.\n",
      "--------------------------------------------------\n",
      "Prompt: \"You are an expert school principal, skilled in effectively managing faculty and staff. Draft 10-15 questions for a potential first grade Head Teacher for my K-12, all-girls', independent school that emphasizes community, joyful discovery, and life-long learning. The candidate is coming in for a first-round panel interview for a 8th grade Math teaching role. They have 5 years of previous teaching experience as an assistant teacher at a co-ed, public school with experience in middle school math teaching. Based on these information, fulfill the following paragraph: The capital of France is\"\n",
      "Generated text: ' the capital of the United Kingdom.\\n\\nThe candidate is coming in for a'\n",
      "Generation time: 0.028345346450805664 seconds.\n",
      "--------------------------------------------------\n",
      "Prompt: \"You are an expert school principal, skilled in effectively managing faculty and staff. Draft 10-15 questions for a potential first grade Head Teacher for my K-12, all-girls', independent school that emphasizes community, joyful discovery, and life-long learning. The candidate is coming in for a first-round panel interview for a 8th grade Math teaching role. They have 5 years of previous teaching experience as an assistant teacher at a co-ed, public school with experience in middle school math teaching. Based on these information, fulfill the following paragraph: The future of AI is\"\n",
      "Generated text: ' in the hands of the teachers.\\n\\nThe candidate is coming in for a'\n",
      "Generation time: 0.028345346450805664 seconds.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "regular_llm = LLM(model=\"facebook/opt-125m\", gpu_memory_utilization=0.4)\n",
    "\n",
    "print(\"Results without `enable_prefix_caching`\")\n",
    "\n",
    "start_time = time.time()\n",
    "outputs = regular_llm.generate(generating_prompts, sampling_params)\n",
    "end_time = time.time()\n",
    "\n",
    "regular_generated_texts = []\n",
    "print(\"-\" * 50)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    regular_generated_texts.append(generated_text)\n",
    "    print(f\"Prompt: {prompt!r}\\nGenerated text: {generated_text!r}\")\n",
    "    print(f\"Generation time: {end_time - start_time} seconds.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b2a6692-2324-471f-8994-1f4559e95147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W905 20:52:46.367828374 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "del regular_llm\n",
    "cleanup_dist_env_and_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cff7ad-133a-4098-b655-65971bdf8582",
   "metadata": {},
   "source": [
    "### Prefix caching enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c4c298c-e389-4b81-8f56-ab52d822f113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:55:57 [config.py:853] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 09-05 20:55:57 [config.py:1467] Using max model len 2048\n",
      "INFO 09-05 20:55:57 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-05 20:55:57 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "INFO 09-05 20:56:00 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-05 20:56:09 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-05 20:56:09 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/opt-125m, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-05 20:56:09 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fcf36473dd0>\n",
      "INFO 09-05 20:56:09 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-05 20:56:10 [gpu_model_runner.py:1751] Starting to load model facebook/opt-125m...\n",
      "INFO 09-05 20:56:10 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-05 20:56:10 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-05 20:56:10 [weight_utils.py:292] Using model weights format ['*.bin']\n",
      "INFO 09-05 20:56:10 [default_loader.py:272] Loading weights took 0.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.87it/s]\n",
      "Loading pt checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.87it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:56:10 [gpu_model_runner.py:1782] Model loading took 0.2500 GiB and 0.318966 seconds\n",
      "INFO 09-05 20:56:12 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/c36373a52c/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-05 20:56:12 [backends.py:520] Dynamo bytecode transform time: 1.36 s\n",
      "INFO 09-05 20:56:13 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 0.812 s\n",
      "INFO 09-05 20:56:13 [monitor.py:34] torch.compile takes 1.36 s in total\n",
      "INFO 09-05 20:56:26 [gpu_worker.py:232] Available KV cache memory: 73.79 GiB\n",
      "INFO 09-05 20:56:26 [kv_cache_utils.py:716] GPU KV cache size: 2,149,280 tokens\n",
      "INFO 09-05 20:56:26 [kv_cache_utils.py:720] Maximum concurrency for 2,048 tokens per request: 1049.45x\n",
      "INFO 09-05 20:56:26 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:06<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 20:56:33 [gpu_model_runner.py:2306] Graph capturing finished in 7 secs, took 0.19 GiB\n",
      "INFO 09-05 20:56:33 [core.py:172] init engine (profile, create kv cache, warmup model) took 22.42 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554ad64c91b442dcbe20d272dbc6092e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a019743812443c9a72b4009d313732f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8e276d72af447993ce966e4cdb50aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9d207cbe784bc1aebeb2b116612640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with `enable_prefix_caching`\n",
      "--------------------------------------------------\n",
      "Prompt: \"You are an expert school principal, skilled in effectively managing faculty and staff. Draft 10-15 questions for a potential first grade Head Teacher for my K-12, all-girls', independent school that emphasizes community, joyful discovery, and life-long learning. The candidate is coming in for a first-round panel interview for a 8th grade Math teaching role. They have 5 years of previous teaching experience as an assistant teacher at a co-ed, public school with experience in middle school math teaching. Based on these information, fulfill the following paragraph: Hello, my name is\"\n",
      "Generated text: ' Sarah, I am a teacher at a private school in the city of New York'\n",
      "Generation time: 0.02359747886657715 seconds.\n",
      "--------------------------------------------------\n",
      "Prompt: \"You are an expert school principal, skilled in effectively managing faculty and staff. Draft 10-15 questions for a potential first grade Head Teacher for my K-12, all-girls', independent school that emphasizes community, joyful discovery, and life-long learning. The candidate is coming in for a first-round panel interview for a 8th grade Math teaching role. They have 5 years of previous teaching experience as an assistant teacher at a co-ed, public school with experience in middle school math teaching. Based on these information, fulfill the following paragraph: The president of the United States is\"\n",
      "Generated text: ' the head of the United States Department of Education. The president of the United States'\n",
      "Generation time: 0.02359747886657715 seconds.\n",
      "--------------------------------------------------\n",
      "Prompt: \"You are an expert school principal, skilled in effectively managing faculty and staff. Draft 10-15 questions for a potential first grade Head Teacher for my K-12, all-girls', independent school that emphasizes community, joyful discovery, and life-long learning. The candidate is coming in for a first-round panel interview for a 8th grade Math teaching role. They have 5 years of previous teaching experience as an assistant teacher at a co-ed, public school with experience in middle school math teaching. Based on these information, fulfill the following paragraph: The capital of France is\"\n",
      "Generated text: ' the capital of the United Kingdom.\\n\\nThe candidate is coming in for a'\n",
      "Generation time: 0.02359747886657715 seconds.\n",
      "--------------------------------------------------\n",
      "Prompt: \"You are an expert school principal, skilled in effectively managing faculty and staff. Draft 10-15 questions for a potential first grade Head Teacher for my K-12, all-girls', independent school that emphasizes community, joyful discovery, and life-long learning. The candidate is coming in for a first-round panel interview for a 8th grade Math teaching role. They have 5 years of previous teaching experience as an assistant teacher at a co-ed, public school with experience in middle school math teaching. Based on these information, fulfill the following paragraph: The future of AI is\"\n",
      "Generated text: ' in the hands of the teachers.\\n\\nThe candidate is coming in for a'\n",
      "Generation time: 0.02359747886657715 seconds.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prefix_cached_llm = LLM(\n",
    "    model=\"facebook/opt-125m\",\n",
    "    enable_prefix_caching=True,\n",
    "    gpu_memory_utilization=0.4,\n",
    ")\n",
    "\n",
    "# Warmup so that the shared prompt's KV cache is computed.\n",
    "prefix_cached_llm.generate(generating_prompts[0], sampling_params)\n",
    "\n",
    "# Generate with prefix caching.\n",
    "start_time = time.time()\n",
    "outputs = prefix_cached_llm.generate(generating_prompts, sampling_params)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Results with `enable_prefix_caching`\")\n",
    "\n",
    "cached_generated_texts = []\n",
    "print(\"-\" * 50)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    cached_generated_texts.append(generated_text)\n",
    "    print(f\"Prompt: {prompt!r}\\nGenerated text: {generated_text!r}\")\n",
    "    print(f\"Generation time: {end_time - start_time} seconds.\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d1408ad-1ca6-4b04-8658-5f7d014301ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answers are the same: True\n"
     ]
    }
   ],
   "source": [
    "generated_same = all(\n",
    "    [\n",
    "        regular_generated_texts[i] == cached_generated_texts[i]\n",
    "        for i in range(len(prompts))\n",
    "    ]\n",
    ")\n",
    "print(f\"Generated answers are the same: {generated_same}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66572324-63e3-43c7-b152-258c1a1709fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W905 20:57:00.193403025 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "del prefix_cached_llm\n",
    "cleanup_dist_env_and_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855eda59-3cf2-4f4a-ace3-bcdff867627f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
