{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3edb1f-f6fa-4ccc-9a53-70a6fe63661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e5850-095c-4774-9d8e-e2301ac0a11c",
   "metadata": {},
   "source": [
    "huggingface permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d127f-4617-4db9-a315-708c0e6af01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8bee7-5603-4ea7-bb89-aaa649468cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bddc014-fed7-4079-ad1c-a77b8a3fcc7f",
   "metadata": {},
   "source": [
    "clean up utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0917d5-8031-483a-84d3-a6fa80550b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import ray\n",
    "import torch\n",
    "try:\n",
    "    del llm\n",
    "    del tokenizer\n",
    "except Exception as e:\n",
    "    print(f\"Failed to unload model: {e}\")\n",
    "finally:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35d27f-8d43-4a52-baf1-d88620b31746",
   "metadata": {},
   "source": [
    "#### generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5dff5-31cc-4d2e-9f2e-24338fcd40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "llm = LLM(model=\"facebook/opt-125m\")\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Geneareted text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4eef61-4367-4aa6-bbd2-9fd099737f98",
   "metadata": {},
   "source": [
    "#### chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6291b-f1a3-4d51-84af-f09778928a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "sampling_params = llm.get_default_sampling_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0d9b6-1c03-4865-83bd-074592cc5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outputs(outputs):\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}\")\n",
    "        print(f\"Generated text: {generated_text!r}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Hello! How can I assist you today?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\":\n",
    "            \"Write an essay about the importance of higher education.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "outputs = llm.chat(conversation, sampling_params, use_tqdm=False)\n",
    "print_outputs(outputs)\n",
    "\n",
    "# batch process\n",
    "conversations = [conversation for _ in range(10)]\n",
    "\n",
    "outputs = llm.chat(conversations, sampling_params, use_tqdm=True)\n",
    "print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee703b06-07c6-412d-9001-dd1365ac7936",
   "metadata": {},
   "source": [
    "#### classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca15c0-2935-4907-8d3f-bd38240e9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=\"jason9693/Qwen2.5-1.5B-apeach\",\n",
    "    task=\"classify\",\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7702ca-02fa-450c-88a9-4f9139061b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "        \"Hello, my name is\",\n",
    "        \"The president of the United States is\",\n",
    "        \"The capital of France is\",\n",
    "        \"The future of AI is\",\n",
    "    ]\n",
    "\n",
    "outputs = llm.classify(prompts)\n",
    "\n",
    "for prompt, output in zip(prompts, outputs):\n",
    "        probs = output.outputs.probs\n",
    "        probs_trimmed = ((str(probs[:16])[:-1] +\n",
    "                          \", ...]\") if len(probs) > 16 else probs)\n",
    "        print(f\"Prompt: {prompt!r} | \"\n",
    "              f\"Class Probabilities: {probs_trimmed} (size={len(probs)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349f50b-5ec3-4f77-a8e3-296f318e66e3",
   "metadata": {},
   "source": [
    "#### score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6477ca0-d3e2-4dd5-9b55-df5d1cfcbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=\"BAAI/bge-reranker-v2-m3\",\n",
    "    task=\"score\",\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caddede-c067-42a3-ba19-3a4ea85c5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"What is the capital of France?\"\n",
    "texts_2 = [\n",
    "    \"The capital of Brazil is Brasilia.\", \"The capital of France is Paris.\"\n",
    "]\n",
    "\n",
    "outputs = model.score(text_1, texts_2)\n",
    "\n",
    "for text_2, output in zip(texts_2, outputs):\n",
    "    score = output.outputs.score\n",
    "    print(f\"Pair: {[text_1, text_2]!r} | Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

