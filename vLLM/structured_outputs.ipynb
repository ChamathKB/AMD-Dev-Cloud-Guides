{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "328f3ec3-2ee5-4b62-8be6-b8efd02faf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e245ce9-dd54-438b-895d-fba4ef6d0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee3e7c-14f1-4a7e-9fed-a81f07b1b6a6",
   "metadata": {},
   "source": [
    "#### Guided decoding by Choice (list of possible options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe7224f-68de-4170-abe7-7cf846814e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_decoding_params_choice = GuidedDecodingParams(choice=[\"Positive\", \"Negative\"])\n",
    "sampling_params_choice = SamplingParams(guided_decoding=guided_decoding_params_choice)\n",
    "\n",
    "prompt_choice = \"Classify this sentiment: vLLM is wonderful!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857090a-2ef0-4719-8342-056032921ae1",
   "metadata": {},
   "source": [
    "#### Guided decoding by Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879e91e7-41f2-44fa-a9e0-c0817631a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_decoding_params_regex = GuidedDecodingParams(regex=r\"\\w+@\\w+\\.com\\n\")\n",
    "sampling_params_regex = SamplingParams(\n",
    "    guided_decoding=guided_decoding_params_regex,\n",
    "    stop=[\"\\n\"],\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "\n",
    "prompt_regex = (\n",
    "    \"Generate an email address for Alan Turing, who works in Enigma.\"\n",
    "    \"End in .com and new line. Example result:\"\n",
    "    \"alan.turing@enigma.com\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b41d57-c158-4ad3-9799-186b7808671a",
   "metadata": {},
   "source": [
    "#### Guided decoding by JSON using Pydantic schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e539bfe7-dd77-408f-870d-2839f6d0e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarType(str, Enum):\n",
    "    sedan = \"sedan\"\n",
    "    suv = \"SUV\"\n",
    "    truck = \"Truck\"\n",
    "    coupe = \"Coupe\"\n",
    "\n",
    "class CarDescription(BaseModel):\n",
    "    brand: str\n",
    "    model: str\n",
    "    car_type: CarType\n",
    "\n",
    "json_schema = CarDescription.model_json_schema()\n",
    "\n",
    "guided_decoding_params_json = GuidedDecodingParams(json=json_schema)\n",
    "sampling_params_json = SamplingParams(\n",
    "    guided_decoding=guided_decoding_params_json,\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "\n",
    "prompt_json = (\n",
    "    \"Generate a JSON with the brand, model and car_type of\"\n",
    "    \"the most iconic car from the 90's\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8047f6-12ca-4adf-be60-3154d2f8064a",
   "metadata": {},
   "source": [
    "#### Guided decoding by Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b60537-d252-453c-bb38-b22b47bdd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_sql_grammar = \"\"\"\n",
    "root ::= select_statement\n",
    "select_statement ::= \"SELECT \" column \" from \" table \" where \" condition\n",
    "column ::= \"col_1 \" | \"col_2 \"\n",
    "table ::= \"table_1 \" | \"table_2 \"\n",
    "condition ::= column \"= \" number\n",
    "number ::= \"1 \" | \"2 \"\n",
    "\"\"\"\n",
    "\n",
    "guided_decoding_params_grammar = GuidedDecodingParams(grammar=simplified_sql_grammar)\n",
    "sampling_params_grammar = SamplingParams(\n",
    "    guided_decoding=guided_decoding_params_grammar,\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "\n",
    "prompt_grammar = (\n",
    "    \"Generate an SQL query to show the 'username' and 'email'from the 'users' table.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9d2fb-178b-4e93-965a-e7078005143f",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccbf121-4962-4e47-be01-5fa66ae66222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(title: str, output: str):\n",
    "    print(f\"{'-' * 50}\\n{title}: {output}\\n{'-' * 50}\")\n",
    "\n",
    "def generate_output(prompt: str, sampling_params: SamplingParams, llm: LLM):\n",
    "    outputs = llm.generate(prompt, sampling_params=sampling_params)\n",
    "    return outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8baeb28c-d7dc-42af-8e11-9fedc75c54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    llm = LLM(model=\"Qwen/Qwen2.5-3B-Instruct\", max_model_len=100)\n",
    "\n",
    "    choice_output = generate_output(prompt_choice, sampling_params_choice, llm)\n",
    "    format_output(\"Guided decoding by Choice\", choice_output)\n",
    "\n",
    "    regex_output = generate_output(prompt_regex, sampling_params_regex, llm)\n",
    "    format_output(\"Guided decoding by Regex\", regex_output)\n",
    "\n",
    "    json_output = generate_output(prompt_json, sampling_params_json, llm)\n",
    "    format_output(\"Guided decoding by JSON\", json_output)\n",
    "\n",
    "    grammar_output = generate_output(prompt_grammar, sampling_params_grammar, llm)\n",
    "    format_output(\"Guided decoding by Grammar\", grammar_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69c9b7b7-21c2-4850-9b27-7b2ee064a978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de20919150574533b81423ef3e8c199a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 21:13:52 [config.py:853] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc3ef824bb642b193c0be877912b02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 21:13:52 [config.py:1467] Using max model len 100\n",
      "INFO 09-05 21:13:59 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-05 21:13:59 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890bfa3461814b66b0b0c7022a31af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802926d434854f38b01f464a9a90ff70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1002394b7b9c4292af13d52419edee82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6e5254a3a04713856a358dcd90b56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-05 21:14:01 [utils.py:2613] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 09-05 21:14:04 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-05 21:14:13 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-05 21:14:13 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='Qwen/Qwen2.5-3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=100, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-05 21:14:13 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x72fc23f87080>\n",
      "INFO 09-05 21:14:13 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 09-05 21:14:13 [rocm.py:338] Model architecture 'Qwen2ForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 09-05 21:14:13 [gpu_model_runner.py:1751] Starting to load model Qwen/Qwen2.5-3B-Instruct...\n",
      "INFO 09-05 21:14:13 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-05 21:14:14 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-05 21:14:14 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-05 21:14:19 [weight_utils.py:308] Time spent downloading weights for Qwen/Qwen2.5-3B-Instruct: 5.538570 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.38s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 21:14:22 [default_loader.py:272] Loading weights took 2.31 seconds\n",
      "INFO 09-05 21:14:22 [gpu_model_runner.py:1782] Model loading took 5.9004 GiB and 8.246731 seconds\n",
      "INFO 09-05 21:14:27 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/fd65d2905e/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-05 21:14:27 [backends.py:520] Dynamo bytecode transform time: 5.32 s\n",
      "INFO 09-05 21:14:47 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 09-05 21:14:47 [backends.py:193] Compiling a graph for general shape takes 17.25 s\n",
      "INFO 09-05 21:14:51 [monitor.py:34] torch.compile takes 22.58 s in total\n",
      "INFO 09-05 21:15:05 [gpu_worker.py:232] Available KV cache memory: 160.23 GiB\n",
      "INFO 09-05 21:15:05 [kv_cache_utils.py:716] GPU KV cache size: 4,666,992 tokens\n",
      "INFO 09-05 21:15:05 [kv_cache_utils.py:720] Maximum concurrency for 100 tokens per request: 41669.57x\n",
      "INFO 09-05 21:15:05 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:15<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 21:15:20 [gpu_model_runner.py:2306] Graph capturing finished in 16 secs, took 0.27 GiB\n",
      "INFO 09-05 21:15:21 [core.py:172] init engine (profile, create kv cache, warmup model) took 58.61 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcd7282ca1e438b98c8e898373057ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029623fdbe7b49679c34dd61b9c4bcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Guided decoding by Choice: Positive\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe4f9b679894a859ff6f5b4076cc553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c1a79b4f874d9c9b65bf4d168a8b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Guided decoding by Regex: alan_turing@endenigma.com\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973cd0f4d4104e3bb3448c7f6fae662e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba783ba7be914b83be4a89e95cd49a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Guided decoding by JSON: {\"brand\": \"Lamborghini\", \"model\": \"Diablo\", \"car_type\": \"Coupe\"}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef667557bd7a40df804f7f5d3616b553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e4a5ad46de4cfba027634efe28c04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Guided decoding by Grammar: SELECT col_1  from table_1  where col_1 = 1 \n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W905 21:15:23.143405678 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c64af4-d9f6-47a9-bd34-7a87f158121e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
