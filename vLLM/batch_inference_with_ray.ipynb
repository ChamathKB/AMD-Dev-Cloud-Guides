{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f65a7f-637e-443a-9d5e-efe4994d4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.llm import build_llm_processor, vLLMEngineProcessorConfig\n",
    "from packaging.version import Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c9dc47-4d47-4e31-ad0b-0d8304e7eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Version(ray.__version__) >= Version(\"2.44.1\"), (\n",
    "    \"Ray version must be at least 2.44.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f1be3-401a-441a-87fd-aea49030962a",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8cfbff-4ed6-403e-8a39-2d2d4a4984b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 10:10:21,434\tINFO worker.py:1852 -- Started a local Ray instance.\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m Max pending requests is set to 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:25:48 [__init__.py:244] Automatically detected platform rocm.\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:26:07 [config.py:1467] Using max model len 16384\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:26:14 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:26:14 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m WARNING 09-05 10:26:15 [utils.py:2613] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:26:18 [__init__.py:244] Automatically detected platform rocm.\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:26:27 [core.py:459] Waiting for init message from front-end.\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:26:27 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='unsloth/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:26:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 16777216, 10, 'psm_d7b239e4'), local_subscribe_addr='ipc:///tmp/f909895b-3ad6-42e0-b86e-b1bd9fa479c0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:26:29 [__init__.py:244] Automatically detected platform rocm.\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m WARNING 09-05 10:26:39 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d2ba461b410>\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_16e208e5'), local_subscribe_addr='ipc:///tmp/4451de57-a70e-4a6b-8453-0caa20aa53dd', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:39 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:39 [gpu_model_runner.py:1751] Starting to load model unsloth/Llama-3.1-8B-Instruct...\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:39 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:39 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:40 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:49 [weight_utils.py:308] Time spent downloading weights for unsloth/Llama-3.1-8B-Instruct: 9.870509 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.82s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:01,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:56 [default_loader.py:272] Loading weights took 6.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.32s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.50s/it]\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:26:56 [gpu_model_runner.py:1782] Model loading took 15.1602 GiB and 16.294464 seconds\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:27:01 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/d3ef4e0109/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:27:01 [backends.py:520] Dynamo bytecode transform time: 5.40 s\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:27:20 [backends.py:181] Cache the graph of shape None for later use\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:27:20 [backends.py:193] Compiling a graph for general shape takes 16.40 s\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:27:23 [monitor.py:34] torch.compile takes 21.80 s in total\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:27:36 [gpu_worker.py:232] Available KV cache memory: 155.92 GiB\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:27:37 [kv_cache_utils.py:716] GPU KV cache size: 1,277,296 tokens\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:27:37 [kv_cache_utils.py:720] Maximum concurrency for 16,384 tokens per request: 77.96x\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:27:37 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs:   0%|          | 0/67 [00:00<?, ?it/s]721)\u001b[0;0m \n",
      "Capturing CUDA graphs:   1%|▏         | 1/67 [00:00<00:59,  1.11it/s]\n",
      "Capturing CUDA graphs:   3%|▎         | 2/67 [00:01<00:32,  2.01it/s]\n",
      "Capturing CUDA graphs:   4%|▍         | 3/67 [00:01<00:23,  2.71it/s]\n",
      "Capturing CUDA graphs:   6%|▌         | 4/67 [00:01<00:19,  3.27it/s]\n",
      "Capturing CUDA graphs:   7%|▋         | 5/67 [00:01<00:16,  3.68it/s]\n",
      "Capturing CUDA graphs:   9%|▉         | 6/67 [00:01<00:15,  4.02it/s]\n",
      "Capturing CUDA graphs:  10%|█         | 7/67 [00:02<00:14,  4.26it/s]\n",
      "Capturing CUDA graphs:  12%|█▏        | 8/67 [00:02<00:13,  4.43it/s]\n",
      "Capturing CUDA graphs:  13%|█▎        | 9/67 [00:02<00:12,  4.58it/s]\n",
      "Capturing CUDA graphs:  15%|█▍        | 10/67 [00:02<00:12,  4.68it/s]\n",
      "Capturing CUDA graphs:  16%|█▋        | 11/67 [00:02<00:11,  4.75it/s]\n",
      "Capturing CUDA graphs:  18%|█▊        | 12/67 [00:03<00:11,  4.75it/s]\n",
      "Capturing CUDA graphs:  19%|█▉        | 13/67 [00:03<00:11,  4.72it/s]\n",
      "Capturing CUDA graphs:  21%|██        | 14/67 [00:03<00:11,  4.73it/s]\n",
      "Capturing CUDA graphs:  22%|██▏       | 15/67 [00:03<00:10,  4.74it/s]\n",
      "Capturing CUDA graphs:  24%|██▍       | 16/67 [00:04<00:10,  4.76it/s]\n",
      "Capturing CUDA graphs:  25%|██▌       | 17/67 [00:04<00:10,  4.77it/s]\n",
      "Capturing CUDA graphs:  27%|██▋       | 18/67 [00:04<00:10,  4.76it/s]\n",
      "Capturing CUDA graphs:  28%|██▊       | 19/67 [00:04<00:10,  4.73it/s]\n",
      "Capturing CUDA graphs:  30%|██▉       | 20/67 [00:04<00:09,  4.74it/s]\n",
      "Capturing CUDA graphs:  31%|███▏      | 21/67 [00:05<00:09,  4.82it/s]\n",
      "Capturing CUDA graphs:  33%|███▎      | 22/67 [00:05<00:09,  4.87it/s]\n",
      "Capturing CUDA graphs:  34%|███▍      | 23/67 [00:05<00:08,  4.89it/s]\n",
      "Capturing CUDA graphs:  36%|███▌      | 24/67 [00:05<00:08,  4.90it/s]\n",
      "Capturing CUDA graphs:  37%|███▋      | 25/67 [00:05<00:08,  4.92it/s]\n",
      "Capturing CUDA graphs:  39%|███▉      | 26/67 [00:06<00:08,  4.93it/s]\n",
      "Capturing CUDA graphs:  40%|████      | 27/67 [00:06<00:08,  4.85it/s]\n",
      "Capturing CUDA graphs:  42%|████▏     | 28/67 [00:06<00:08,  4.86it/s]\n",
      "Capturing CUDA graphs:  43%|████▎     | 29/67 [00:06<00:07,  4.88it/s]\n",
      "Capturing CUDA graphs:  45%|████▍     | 30/67 [00:06<00:07,  4.92it/s]\n",
      "Capturing CUDA graphs:  46%|████▋     | 31/67 [00:07<00:07,  4.95it/s]\n",
      "Capturing CUDA graphs:  48%|████▊     | 32/67 [00:07<00:07,  4.97it/s]\n",
      "Capturing CUDA graphs:  49%|████▉     | 33/67 [00:07<00:06,  5.00it/s]\n",
      "Capturing CUDA graphs:  51%|█████     | 34/67 [00:07<00:06,  4.96it/s]\n",
      "Capturing CUDA graphs:  52%|█████▏    | 35/67 [00:07<00:06,  4.97it/s]\n",
      "Capturing CUDA graphs:  54%|█████▎    | 36/67 [00:08<00:06,  5.01it/s]\n",
      "Capturing CUDA graphs:  55%|█████▌    | 37/67 [00:08<00:05,  5.03it/s]\n",
      "Capturing CUDA graphs:  57%|█████▋    | 38/67 [00:08<00:05,  5.03it/s]\n",
      "Capturing CUDA graphs:  58%|█████▊    | 39/67 [00:08<00:05,  5.04it/s]\n",
      "Capturing CUDA graphs:  60%|█████▉    | 40/67 [00:08<00:05,  5.01it/s]\n",
      "Capturing CUDA graphs:  61%|██████    | 41/67 [00:09<00:05,  5.05it/s]\n",
      "Capturing CUDA graphs:  63%|██████▎   | 42/67 [00:09<00:04,  5.09it/s]\n",
      "Capturing CUDA graphs:  64%|██████▍   | 43/67 [00:09<00:04,  5.10it/s]\n",
      "Capturing CUDA graphs:  66%|██████▌   | 44/67 [00:09<00:04,  5.11it/s]\n",
      "Capturing CUDA graphs:  67%|██████▋   | 45/67 [00:09<00:04,  5.10it/s]\n",
      "Capturing CUDA graphs:  69%|██████▊   | 46/67 [00:10<00:04,  5.11it/s]\n",
      "Capturing CUDA graphs:  70%|███████   | 47/67 [00:10<00:03,  5.12it/s]\n",
      "Capturing CUDA graphs:  72%|███████▏  | 48/67 [00:10<00:03,  5.13it/s]\n",
      "Capturing CUDA graphs:  73%|███████▎  | 49/67 [00:10<00:03,  5.12it/s]\n",
      "Capturing CUDA graphs:  75%|███████▍  | 50/67 [00:11<00:06,  2.66it/s]\n",
      "Capturing CUDA graphs:  76%|███████▌  | 51/67 [00:11<00:05,  3.08it/s]\n",
      "Capturing CUDA graphs:  78%|███████▊  | 52/67 [00:11<00:04,  3.50it/s]\n",
      "Capturing CUDA graphs:  79%|███████▉  | 53/67 [00:12<00:03,  3.88it/s]\n",
      "Capturing CUDA graphs:  81%|████████  | 54/67 [00:12<00:03,  4.17it/s]\n",
      "Capturing CUDA graphs:  82%|████████▏ | 55/67 [00:12<00:02,  4.43it/s]\n",
      "Capturing CUDA graphs:  84%|████████▎ | 56/67 [00:12<00:02,  4.59it/s]\n",
      "Capturing CUDA graphs:  85%|████████▌ | 57/67 [00:12<00:02,  4.75it/s]\n",
      "Capturing CUDA graphs:  87%|████████▋ | 58/67 [00:13<00:01,  4.89it/s]\n",
      "Capturing CUDA graphs:  88%|████████▊ | 59/67 [00:13<00:01,  4.94it/s]\n",
      "Capturing CUDA graphs:  90%|████████▉ | 60/67 [00:13<00:01,  4.95it/s]\n",
      "Capturing CUDA graphs:  91%|█████████ | 61/67 [00:13<00:01,  4.97it/s]\n",
      "Capturing CUDA graphs:  93%|█████████▎| 62/67 [00:13<00:01,  4.97it/s]\n",
      "Capturing CUDA graphs:  94%|█████████▍| 63/67 [00:14<00:00,  5.00it/s]\n",
      "Capturing CUDA graphs:  96%|█████████▌| 64/67 [00:14<00:00,  4.95it/s]\n",
      "Capturing CUDA graphs:  97%|█████████▋| 65/67 [00:14<00:00,  4.94it/s]\n",
      "Capturing CUDA graphs:  99%|█████████▊| 66/67 [00:14<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=360)\u001b[0m \u001b[1;36m(VllmWorker rank=0 pid=1721)\u001b[0;0m INFO 09-05 10:27:52 [gpu_model_runner.py:2306] Graph capturing finished in 16 secs, took 0.27 GiB\n",
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:27:52 [core.py:172] init engine (profile, create kv cache, warmup model) took 56.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:15<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=360)\u001b[0m INFO 09-05 10:27:53 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 79831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 4a856ebea684496e82850a3b037ad257 with size 64: 0.4241426239998418\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 80e25b8aaf0142d0bf47054253884fba with size 64: 0.4242875229999754\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch fb4f5e2e8a944f49a4a008c16ac1f7ff with size 17: 0.6250052119999054\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch f8ac32b0028d4df881ecd8d5a00e1e7c with size 64: 0.8099361740000859\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch afc6853fc3844e56a4bacd28da4aabae with size 64: 0.8104278639998483\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 7efb400d57c44080a7b6ea35ac571c0b with size 17: 0.8107645620000312\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 219cfd76a55d4606aaa1cec22b037e7b with size 64: 0.6318103940000128\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch b5298f59733741e2846049752f8c8dc9 with size 17: 0.9064981770000031\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 9ffac9914dad4a098fdd085e6f1ce545 with size 64: 0.9235466690001886\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 4560292887e042db88077b5d94610ee7 with size 64: 0.7999911100000645\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch a3bf0b3d738b468783ab5bfea6bcccd7 with size 64: 1.011383474000013\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch ec2caa1e5e174e1283d3b9d995df2116 with size 17: 1.028592251000191\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 9d54436f790a4d00bdeabb2b3400f0de with size 64: 0.6047821100000874\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 418555338ce548898ca904ece508999a with size 64: 0.8201167129998339\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch b3b328b76f35499f9f017380a56b3cd4 with size 17: 0.8206353440000385\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 427c60cd4b954658a2890e94999dc420 with size 64: 0.896780723999882\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 10709c85f85b40cfa4af21c6c5ad63b2 with size 64: 0.9889125590000276\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 447e6a87569f45409b98fbccfd692463 with size 17: 1.0109273759999269\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch d9783a5c6b9d4afab12bf5ea71aaa10b with size 64: 0.8688812479999797\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch db85321b7cf442e19e3886294ec8755a with size 64: 0.9185675480000555\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 7a2900491f4d47c2baff3d90ba6b40ad with size 17: 0.9464197499999045\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 1911d8b9d1ee46e08c53458aaffe016d with size 64: 0.9494438589999845\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 862ab35206ea41ee94a12caee532011f with size 64: 0.9626342350002233\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m [vLLM] Elapsed time for batch 43e153ab90c84079b290ae3aa4ae7456 with size 17: 0.9937831130000632\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=360)\u001b[0m Shutting down vLLM engine\n"
     ]
    }
   ],
   "source": [
    "ds = ray.data.read_text(\"s3://anonymous@air-example-data/prompts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d09b86-d7db-4226-8d46-275b34bdcc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 10:10:40,148\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-09-05_10-10-20_593878_45/logs/ray-data\n",
      "2025-09-05 10:10:40,149\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadText]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d7b29cb8ac4a648507a9019789f7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3883e51645642cd81dd9c44303a5939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadText->SplitBlocks(40) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column  Type\n",
      "------  ----\n",
      "text    string\n"
     ]
    }
   ],
   "source": [
    "print(ds.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aaec124-4150-4d71-a546-c938cb47a42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 10:11:33,301\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-09-05_10-10-20_593878_45/logs/ray-data\n",
      "2025-09-05 10:11:33,301\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadText] -> AggregateNumRows[AggregateNumRows]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d9a6e7bb3d4c36b39cb5505b090a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecceaba0ea8b4be2aaed552347528d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadText->SplitBlocks(40) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c7014dd2b74e3da41502ca517a5755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- AggregateNumRows 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 5800 prompts\n"
     ]
    }
   ],
   "source": [
    "size = ds.count()\n",
    "print(f\"Size of dataset: {size} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c779776-5d9f-4013-8329-5c0f22542f05",
   "metadata": {},
   "source": [
    "### Configure vLLM engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f23bb2-cd0c-4338-875a-e9506bafb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    engine_kwargs={\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"max_num_batched_tokens\": 4096,\n",
    "        \"max_model_len\": 16384,\n",
    "    },\n",
    "    concurrency=1,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b418ce-c146-40ee-9ccb-2634ecc57752",
   "metadata": {},
   "source": [
    "### Processor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c775a83-44d6-4efc-a574-4505cf6dffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 10:22:50,394\tINFO worker.py:1684 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea768b195064a498451825545dd81aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/896 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vllm_processor = build_llm_processor(\n",
    "    config,\n",
    "    preprocess=lambda row: dict(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a bot that responds with haikus.\"},\n",
    "            {\"role\": \"user\", \"content\": row[\"text\"]},\n",
    "        ],\n",
    "        sampling_params=dict(\n",
    "            temperature=0.3,\n",
    "            max_tokens=250,\n",
    "        ),\n",
    "    ),\n",
    "    postprocess=lambda row: dict(\n",
    "        answer=row[\"generated_text\"],\n",
    "        **row,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a13463-5c12-4c67-b216-7c7bf45fa0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = vllm_processor(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96fc41-60cd-46e8-8f4f-c401e47a264c",
   "metadata": {},
   "source": [
    "#### Demo\n",
    "Testing and debugging with 10 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f9b433-c114-4919-9fd4-23e01ce771d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 10:25:37,505\tINFO dataset.py:2796 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2025-09-05 10:25:37,511\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-09-05_10-10-20_593878_45/logs/ray-data\n",
      "2025-09-05 10:25:37,511\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadText] -> ActorPoolMapOperator[Map(_preprocess)->MapBatches(ChatTemplateUDF)] -> ActorPoolMapOperator[MapBatches(TokenizeUDF)] -> ActorPoolMapOperator[MapBatches(vLLMEngineStageUDF)] -> ActorPoolMapOperator[MapBatches(DetokenizeUDF)] -> TaskPoolMapOperator[Map(_postprocess)] -> LimitOperator[limit=10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc93da69bd574d799ecb59f386aaf40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ff4fc1759546d4b0d75140aede201f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadText->SplitBlocks(40) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960e164d401248f48776a62ba8e2e3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_preprocess)->MapBatches(ChatTemplateUDF) 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc237c17bde40db80d69e3971478e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TokenizeUDF) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a517a8054b48508c06e4774a53ff0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(vLLMEngineStageUDF) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01f3c4ba7034d81aa0dcb3a61fe5a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(DetokenizeUDF) 5: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a05a56cb75449b8542645d0e0e236d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_postprocess) 6: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddd632df1b5440ba528bbd12521b1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- limit=10 7: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe best way to learn a new language is...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: ('Listen and repeat\\nWords in context, not rules cold\\nPractice every day',)\n",
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe biggest challenge facing our society today is...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: (\"Climate's heavy weight\\nRising seas and burning earth\\nFuture's uncertain\",)\n",
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe happiest moment of my life was...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: ('Laughter echoes free\\nGolden sun on smiling face\\nJoy in pure delight',)\n",
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIf I could have any piece of technology, it would be a...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: ('Time machine device\\nTravel through the ages freely\\nKnowledge at my grasp',)\n",
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIf I could have any type of technology, it would be a...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: ('Time machine found\\nTravel through the ages free\\nKnowledge at my feet',)\n",
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe best book I ever read was...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: ('Pages turned with care\\nWords that danced upon my soul\\nLove that never fades',)\n",
      "Prompt: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe most interesting place I've ever been to is...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
      "Generated text: ('Ancient stones whisper\\nEchoes of secrets past sleep\\nMystery abides',)\n",
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIf I could have any talent, it would be...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: ('Musical whispers sweet\\nGolden notes upon my lips\\nEchoing the soul',)\n",
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIf I could have any type of career, it would be a...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: ('Cosmic cartographer\\nMapping stars and galaxies wide\\nMystery unfurls',)\n",
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a bot that responds with haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIf I could have any type of superpower, it would be...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
      "Generated text: ('Time to rewind\\nMoments of regret and pain\\nForgiveness unfurls',)\n"
     ]
    }
   ],
   "source": [
    "outputs = ds.take(limit=10)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output[\"prompt\"]\n",
    "    generated_text = output[\"generated_text\"],\n",
    "    print(f\"Prompt: {prompt!r}\")\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa66203-fb5a-43b6-86af-3988cdfd0e43",
   "metadata": {},
   "source": [
    "#### In production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413c1e2-7274-4a70-a912-2f0cb10e1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.write_parquet(\"s3://<your-output-bucket>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
