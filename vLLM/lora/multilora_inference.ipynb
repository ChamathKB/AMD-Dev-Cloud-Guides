{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb607af-2b6a-408c-bc7c-e9794fcaacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b35d1-0fc9-40d0-a090-de805725c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4eab51-0aa1-41fb-a862-ae0ac78104d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63eb52-9a5d-42c5-8db0-38d19cfe577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompts(\n",
    "    lora_path: str,\n",
    ") -> list[tuple[str, SamplingParams, Optional[LoRARequest]]]:\n",
    "    \"\"\"Create a list of test prompts with their sampling parameters.\n",
    "\n",
    "    2 requests for base model, 4 requests for the LoRA. We define 2\n",
    "    different LoRA adapters (using the same model for demo purposes).\n",
    "    Since we also set `max_loras=1`, the expectation is that the requests\n",
    "    with the second LoRA adapter will be ran after all requests with the\n",
    "    first adapter have finished.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        (\n",
    "            \"A robot may not injure a human being\",\n",
    "            SamplingParams(\n",
    "                temperature=0.0, logprobs=1, prompt_logprobs=1, max_tokens=128\n",
    "            ),\n",
    "            None,\n",
    "        ),\n",
    "        (\n",
    "            \"To be or not to be,\",\n",
    "            SamplingParams(\n",
    "                temperature=0.8, top_k=5, presence_penalty=0.2, max_tokens=128\n",
    "            ),\n",
    "            None,\n",
    "        ),\n",
    "        (\n",
    "            \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",  # noqa: E501\n",
    "            SamplingParams(\n",
    "                temperature=0.0,\n",
    "                logprobs=1,\n",
    "                prompt_logprobs=1,\n",
    "                max_tokens=128,\n",
    "                stop_token_ids=[32003],\n",
    "            ),\n",
    "            LoRARequest(\"sql-lora\", 1, lora_path),\n",
    "        ),\n",
    "        (\n",
    "            \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",  # noqa: E501\n",
    "            SamplingParams(\n",
    "                temperature=0.0,\n",
    "                logprobs=1,\n",
    "                prompt_logprobs=1,\n",
    "                max_tokens=128,\n",
    "                stop_token_ids=[32003],\n",
    "            ),\n",
    "            LoRARequest(\"sql-lora2\", 2, lora_path),\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17676f4e-8388-4d9f-8e68-dd7b800d2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_requests(\n",
    "    engine: LLMEngine,\n",
    "    test_prompts: list[tuple[str, SamplingParams, Optional[LoRARequest]]],\n",
    "):\n",
    "    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\n",
    "    request_id = 0\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    while test_prompts or engine.has_unfinished_requests():\n",
    "        if test_prompts:\n",
    "            prompt, sampling_params, lora_request = test_prompts.pop(0)\n",
    "            engine.add_request(\n",
    "                str(request_id), prompt, sampling_params, lora_request=lora_request\n",
    "            )\n",
    "            request_id += 1\n",
    "\n",
    "        request_outputs: list[RequestOutput] = engine.step()\n",
    "\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                print(request_output)\n",
    "                print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7421f-913f-4cfd-97ad-fb515ad3fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_engine() -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine.\"\"\"\n",
    "    # max_loras: controls the number of LoRAs that can be used in the same\n",
    "    #   batch. Larger numbers will cause higher memory usage, as each LoRA\n",
    "    #   slot requires its own preallocated tensor.\n",
    "    # max_lora_rank: controls the maximum supported rank of all LoRAs. Larger\n",
    "    #   numbers will cause higher memory usage. If you know that all LoRAs will\n",
    "    #   use the same rank, it is recommended to set this as low as possible.\n",
    "    # max_cpu_loras: controls the size of the CPU LoRA cache.\n",
    "    engine_args = EngineArgs(\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        enable_lora=True,\n",
    "        max_loras=1,\n",
    "        max_lora_rank=8,\n",
    "        max_cpu_loras=2,\n",
    "        max_num_seqs=256,\n",
    "    )\n",
    "    return LLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db37362-9ff7-45ad-a177-abb17e91a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function that sets up and runs the prompt processing.\"\"\"\n",
    "    engine = initialize_engine()\n",
    "    lora_path = snapshot_download(repo_id=\"irisaparina/llama-3-8b-instruct-ambrosia-infilling-lora\")\n",
    "    test_prompts = create_test_prompts(lora_path)\n",
    "    process_requests(engine, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c718aed9-84bb-408e-9fb4-37360e9944f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71082a37-262d-4643-b7c9-535907268e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
