{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d906a7-1684-43c2-af7a-9195dee02084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:36:12 [__init__.py:244] Automatically detected platform rocm.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "from vllm import LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74977528-99dd-40e1-a706-73c9ec6c11bb",
   "metadata": {},
   "source": [
    "#### Hugging Face API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2632ef4a-9a28-4afa-8f66-a575e4e8e37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea911219e9940c0aa10a7c12aabaaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c385bc81-a506-4ce1-9d8d-0ae2dfcef3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token validated successfully! Logged in as: Chamath\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690c273-711c-4d06-823c-8527ee8db4f9",
   "metadata": {},
   "source": [
    "### Initilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15c2d0e-84a3-47a9-99a5-5ec9d8f3e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer_and_llm(model_name: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    transformers_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    embedding_layer = transformers_model.get_input_embeddings()\n",
    "    llm = LLM(model=model_name, enable_prompt_embeds=True)\n",
    "    return tokenizer, embedding_layer, llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb0947-3704-435d-a336-202dfa53fa60",
   "metadata": {},
   "source": [
    "### Prompt Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b800ad0a-f737-441b-b66c-805a59d3f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_embeds(\n",
    "    chat: list[dict[str, str]],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    embedding_layer: torch.nn.Module,\n",
    "):\n",
    "    token_ids = tokenizer.apply_chat_template(\n",
    "        chat, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    prompt_embeds = embedding_layer(token_ids).squeeze(0)\n",
    "    return prompt_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12765d8-2577-4318-a38b-e5bc96e96861",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40384e6f-28ba-4f55-b971-8a1ef2ce6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_prompt_inference(\n",
    "    llm: LLM, tokenizer: PreTrainedTokenizer, embedding_layer: torch.nn.Module\n",
    "):\n",
    "    chat = [{\"role\": \"user\", \"content\": \"Please tell me about the capital of France.\"}]\n",
    "    prompt_embeds = get_prompt_embeds(chat, tokenizer, embedding_layer)\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        {\n",
    "            \"prompt_embeds\": prompt_embeds,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"\\n[Single Inference Output]\")\n",
    "    print(\"-\" * 30)\n",
    "    for o in outputs:\n",
    "        print(o.outputs[0].text)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bde572-d71d-44de-a236-12dc51c854b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prompt_inference(\n",
    "    llm: LLM, tokenizer: PreTrainedTokenizer, embedding_layer: torch.nn.Module\n",
    "):\n",
    "    chats = [\n",
    "        [{\"role\": \"user\", \"content\": \"Please tell me about the capital of France.\"}],\n",
    "        [{\"role\": \"user\", \"content\": \"When is the day longest during the year?\"}],\n",
    "        [{\"role\": \"user\", \"content\": \"Where is bigger, the moon or the sun?\"}],\n",
    "    ]\n",
    "\n",
    "    prompt_embeds_list = [\n",
    "        get_prompt_embeds(chat, tokenizer, embedding_layer) for chat in chats\n",
    "    ]\n",
    "\n",
    "    outputs = llm.generate([{\"prompt_embeds\": embeds} for embeds in prompt_embeds_list])\n",
    "\n",
    "    print(\"\\n[Batch Inference Outputs]\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, o in enumerate(outputs):\n",
    "        print(f\"Q{i + 1}: {chats[i][0]['content']}\")\n",
    "        print(f\"A{i + 1}: {o.outputs[0].text}\\n\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5603e3-0fe6-4b87-8759-bb872962a8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752bf2117dbe4661aaeee53c5d4940e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f5025114d34380a77a4ee83fe0fbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a90d469c0d462181ffababa7ae694b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac702e76c0274206a8a6cf4401eda038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8360db9ee044c8bd51c15ed40f6a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a91d2a12a214534bfb6e33debfa15c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17dc1f4a763417797725188bff76d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61944ebcb02942ba8593b381c45821c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c24b964fd748358b57613d2d9a0684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85791245c62043bfbffb155c5534f5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45e6a997778419cbd81cee67c88c686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0135f15d28834d8bb7628dd9803cb82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f6f45be0364c229300080d7cab839f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:39:27 [config.py:853] This model supports multiple tasks: {'generate', 'score', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 09-26 20:39:27 [config.py:1467] Using max model len 32768\n",
      "WARNING 09-26 20:39:27 [arg_utils.py:1719] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. \n",
      "INFO 09-26 20:39:35 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "INFO 09-26 20:39:35 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='Qwen/Qwen2-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 09-26 20:39:36 [rocm.py:233] Using ROCmFlashAttention backend.\n",
      "INFO 09-26 20:39:36 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-26 20:39:36 [model_runner.py:1171] Starting to load model Qwen/Qwen2-7B-Instruct...\n",
      "WARNING 09-26 20:39:36 [rocm.py:338] Model architecture 'Qwen2ForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 09-26 20:39:37 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e090ba7607c4108afdab5a6ae64fb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:39:42 [default_loader.py:272] Loading weights took 5.50 seconds\n",
      "INFO 09-26 20:39:43 [model_runner.py:1203] Model loading took 14.5020 GiB and 5.967543 seconds\n",
      "INFO 09-26 20:40:11 [worker.py:294] Memory profiling takes 27.96 seconds\n",
      "INFO 09-26 20:40:11 [worker.py:294] the current vLLM instance can use total_gpu_memory (191.69GiB) x gpu_memory_utilization (0.90) = 172.52GiB\n",
      "INFO 09-26 20:40:11 [worker.py:294] model weights take 14.50GiB; non_torch_memory takes 0.71GiB; PyTorch activation peak memory takes 4.76GiB; the rest of the memory reserved for KV Cache is 152.55GiB.\n",
      "INFO 09-26 20:40:11 [executor_base.py:113] # rocm blocks: 178521, # CPU blocks: 4681\n",
      "INFO 09-26 20:40:11 [executor_base.py:118] Maximum concurrency for 32768 tokens per request: 87.17x\n",
      "INFO 09-26 20:40:12 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe14d376d3b4f44bcef0e4713b87bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:40:31 [model_runner.py:1671] Graph capturing finished in 19 secs, took 0.39 GiB\n",
      "INFO 09-26 20:40:31 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 48.63 seconds\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "tokenizer, embedding_layer, llm = init_tokenizer_and_llm(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "025d416f-daf0-4edd-9ec2-493de28d4d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-26 20:41:52 [config.py:1394] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ff53f9c73542b18914a444f3c1a30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430c80cea0204961929cff61f863ba90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Single Inference Output]\n",
      "------------------------------\n",
      "The capital of France is Paris. It is located in the northern central part of\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "single_prompt_inference(llm, tokenizer, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee057919-1018-471e-b81e-5a96d8f41b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59ce1ba9dd84303a6b0b05f668aead5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4adb8ee4aff4564b364f3ee3107fcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Batch Inference Outputs]\n",
      "------------------------------\n",
      "Q1: Please tell me about the capital of France.\n",
      "A1: The capital of France is Paris. It is located in the northern central part of\n",
      "\n",
      "Q2: When is the day longest during the year?\n",
      "A2: The day is longest during the year on the summer solstice. This typically occurs\n",
      "\n",
      "Q3: Where is bigger, the moon or the sun?\n",
      "A3: The Sun is much bigger than the Moon. Despite the Moon being the largest satellite\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_prompt_inference(llm, tokenizer, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38a784-aa4a-4297-9166-45d5bc03c20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
