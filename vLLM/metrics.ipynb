{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267c13da-b01b-4c4a-874b-4ecf42595d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:09:04 [__init__.py:244] Automatically detected platform rocm.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.v1.metrics.reader import Counter, Gauge, Histogram, Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0684c73-c858-41b1-897a-24d405df0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcdf9907-913a-4b83-a915-2f9cb39cd8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a28ca1-7796-4098-97f3-8a5cd2bce826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    llm = LLM(model=\"Qwen/Qwen3-30B-A3B\", disable_log_stats=False)\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}\\nGenerated text: {generated_text!r}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    for metric in llm.get_metrics():\n",
    "        if isinstance(metric, Gauge):\n",
    "            print(f\"{metric.name} (gauge) = {metric.value}\")\n",
    "        elif isinstance(metric, Counter):\n",
    "            print(f\"{metric.name} (counter) = {metric.value}\")\n",
    "        elif isinstance(metric, Vector):\n",
    "            print(f\"{metric.name} (vector) = {metric.values}\")\n",
    "        elif isinstance(metric, Histogram):\n",
    "            print(f\"{metric.name} (histogram)\")\n",
    "            print(f\"    sum = {metric.sum}\")\n",
    "            print(f\"    count = {metric.count}\")\n",
    "            for bucket_le, value in metric.buckets.items():\n",
    "                print(f\"    {bucket_le} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfd81d7-0a92-4d03-aec4-5f40ba396656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fa67fd681340c3b2ee10d8b847a568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:15:29 [config.py:853] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward', 'score'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc93662185e4e8e81e34e4f008f3107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:15:29 [config.py:1467] Using max model len 40960\n",
      "INFO 09-26 20:15:29 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-26 20:15:29 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aff0db6b874d998aea6d1a765a7f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6db25389a54b63a3800774586cbb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4438b4ee9d46ed9320e4fa43223ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b126e1aec6941deba7a99be9794991e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:15:34 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-26 20:15:45 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-26 20:15:45 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='Qwen/Qwen3-30B-A3B', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-26 20:15:45 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x77f40af82ae0>\n",
      "INFO 09-26 20:15:45 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-26 20:15:45 [gpu_model_runner.py:1751] Starting to load model Qwen/Qwen3-30B-A3B...\n",
      "INFO 09-26 20:15:45 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-26 20:15:45 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-26 20:15:46 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-26 20:16:34 [weight_utils.py:308] Time spent downloading weights for Qwen/Qwen3-30B-A3B: 48.120694 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:01<00:23,  1.58s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:03<00:23,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:05<00:21,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:06<00:20,  1.70s/it]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:07<00:14,  1.30s/it]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:08<00:13,  1.38s/it]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:10<00:13,  1.51s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:12<00:12,  1.57s/it]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:14<00:11,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:15<00:09,  1.65s/it]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:17<00:08,  1.67s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 12/16 [00:19<00:06,  1.69s/it]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 13/16 [00:20<00:05,  1.70s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 14/16 [00:22<00:03,  1.70s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 15/16 [00:24<00:01,  1.70s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:26<00:00,  1.70s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:26<00:00,  1.63s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:17:00 [default_loader.py:272] Loading weights took 26.27 seconds\n",
      "INFO 09-26 20:17:01 [gpu_model_runner.py:1782] Model loading took 77.5273 GiB and 74.944923 seconds\n",
      "INFO 09-26 20:17:08 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/fe72b23fcf/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-26 20:17:08 [backends.py:520] Dynamo bytecode transform time: 7.15 s\n",
      "INFO 09-26 20:17:38 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 09-26 20:17:38 [backends.py:193] Compiling a graph for general shape takes 27.48 s\n",
      "INFO 09-26 20:17:46 [monitor.py:34] torch.compile takes 34.63 s in total\n",
      "WARNING 09-26 20:17:57 [fused_moe.py:683] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=AMD_Instinct_MI300X.json\n",
      "INFO 09-26 20:17:59 [gpu_worker.py:232] Available KV cache memory: 88.58 GiB\n",
      "INFO 09-26 20:18:00 [kv_cache_utils.py:716] GPU KV cache size: 967,568 tokens\n",
      "INFO 09-26 20:18:00 [kv_cache_utils.py:720] Maximum concurrency for 40,960 tokens per request: 23.62x\n",
      "INFO 09-26 20:18:00 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:26<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-26 20:18:26 [gpu_model_runner.py:2306] Graph capturing finished in 26 secs, took 0.40 GiB\n",
      "INFO 09-26 20:18:26 [core.py:172] init engine (profile, create kv cache, warmup model) took 85.48 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3381dab44910400fb46a3ef4c94d4a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22435ecf8e004b01ba19d0f25915dcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Prompt: 'Hello, my name is'\n",
      "Generated text: \" Jamie and I'm a new user to the team, so I'll start with\"\n",
      "--------------------------------------------------\n",
      "Prompt: 'The president of the United States is'\n",
      "Generated text: ' the most powerful person in the world, but what would happen if someone stole his'\n",
      "--------------------------------------------------\n",
      "Prompt: 'The capital of France is'\n",
      "Generated text: ' Paris. What is the capital of Germany? The capital of Germany is Berlin.\\n\\n'\n",
      "--------------------------------------------------\n",
      "Prompt: 'The future of AI is'\n",
      "Generated text: ' being defined in a new way as the world’s top AI researchers and companies gather'\n",
      "--------------------------------------------------\n",
      "vllm:num_requests_running (gauge) = 0.0\n",
      "vllm:num_requests_waiting (gauge) = 0.0\n",
      "vllm:gpu_cache_usage_perc (gauge) = 1.6536305458614287e-05\n",
      "vllm:gpu_prefix_cache_queries (counter) = 22\n",
      "vllm:gpu_prefix_cache_hits (counter) = 0\n",
      "vllm:kv_cache_usage_perc (gauge) = 1.6536305458614287e-05\n",
      "vllm:prefix_cache_queries (counter) = 22\n",
      "vllm:prefix_cache_hits (counter) = 0\n",
      "vllm:num_preemptions (counter) = 0\n",
      "vllm:prompt_tokens (counter) = 22\n",
      "vllm:generation_tokens (counter) = 64\n",
      "vllm:request_success (counter) = 0\n",
      "vllm:request_success (counter) = 4\n",
      "vllm:request_success (counter) = 0\n",
      "vllm:request_prompt_tokens (histogram)\n",
      "    sum = 22.0\n",
      "    count = 4\n",
      "    1.0 = 0\n",
      "    2.0 = 0\n",
      "    5.0 = 3\n",
      "    10.0 = 4\n",
      "    20.0 = 4\n",
      "    50.0 = 4\n",
      "    100.0 = 4\n",
      "    200.0 = 4\n",
      "    500.0 = 4\n",
      "    1000.0 = 4\n",
      "    2000.0 = 4\n",
      "    5000.0 = 4\n",
      "    10000.0 = 4\n",
      "    20000.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:request_generation_tokens (histogram)\n",
      "    sum = 64.0\n",
      "    count = 4\n",
      "    1.0 = 0\n",
      "    2.0 = 0\n",
      "    5.0 = 0\n",
      "    10.0 = 0\n",
      "    20.0 = 4\n",
      "    50.0 = 4\n",
      "    100.0 = 4\n",
      "    200.0 = 4\n",
      "    500.0 = 4\n",
      "    1000.0 = 4\n",
      "    2000.0 = 4\n",
      "    5000.0 = 4\n",
      "    10000.0 = 4\n",
      "    20000.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:iteration_tokens_total (histogram)\n",
      "    sum = 86.0\n",
      "    count = 17\n",
      "    1.0 = 0\n",
      "    8.0 = 16\n",
      "    16.0 = 16\n",
      "    32.0 = 17\n",
      "    64.0 = 17\n",
      "    128.0 = 17\n",
      "    256.0 = 17\n",
      "    512.0 = 17\n",
      "    1024.0 = 17\n",
      "    2048.0 = 17\n",
      "    4096.0 = 17\n",
      "    8192.0 = 17\n",
      "    16384.0 = 17\n",
      "    +Inf = 17\n",
      "vllm:request_max_num_generation_tokens (histogram)\n",
      "    sum = 64.0\n",
      "    count = 4\n",
      "    1.0 = 0\n",
      "    2.0 = 0\n",
      "    5.0 = 0\n",
      "    10.0 = 0\n",
      "    20.0 = 4\n",
      "    50.0 = 4\n",
      "    100.0 = 4\n",
      "    200.0 = 4\n",
      "    500.0 = 4\n",
      "    1000.0 = 4\n",
      "    2000.0 = 4\n",
      "    5000.0 = 4\n",
      "    10000.0 = 4\n",
      "    20000.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:request_params_n (histogram)\n",
      "    sum = 4.0\n",
      "    count = 4\n",
      "    1.0 = 4\n",
      "    2.0 = 4\n",
      "    5.0 = 4\n",
      "    10.0 = 4\n",
      "    20.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:request_params_max_tokens (histogram)\n",
      "    sum = 64.0\n",
      "    count = 4\n",
      "    1.0 = 0\n",
      "    2.0 = 0\n",
      "    5.0 = 0\n",
      "    10.0 = 0\n",
      "    20.0 = 4\n",
      "    50.0 = 4\n",
      "    100.0 = 4\n",
      "    200.0 = 4\n",
      "    500.0 = 4\n",
      "    1000.0 = 4\n",
      "    2000.0 = 4\n",
      "    5000.0 = 4\n",
      "    10000.0 = 4\n",
      "    20000.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:time_to_first_token_seconds (histogram)\n",
      "    sum = 0.1793677806854248\n",
      "    count = 4\n",
      "    0.001 = 0\n",
      "    0.005 = 0\n",
      "    0.01 = 0\n",
      "    0.02 = 0\n",
      "    0.04 = 1\n",
      "    0.06 = 4\n",
      "    0.08 = 4\n",
      "    0.1 = 4\n",
      "    0.25 = 4\n",
      "    0.5 = 4\n",
      "    0.75 = 4\n",
      "    1.0 = 4\n",
      "    2.5 = 4\n",
      "    5.0 = 4\n",
      "    7.5 = 4\n",
      "    10.0 = 4\n",
      "    20.0 = 4\n",
      "    40.0 = 4\n",
      "    80.0 = 4\n",
      "    160.0 = 4\n",
      "    640.0 = 4\n",
      "    2560.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:time_per_output_token_seconds (histogram)\n",
      "    sum = 0.646213183994405\n",
      "    count = 60\n",
      "    0.01 = 0\n",
      "    0.025 = 60\n",
      "    0.05 = 60\n",
      "    0.075 = 60\n",
      "    0.1 = 60\n",
      "    0.15 = 60\n",
      "    0.2 = 60\n",
      "    0.3 = 60\n",
      "    0.4 = 60\n",
      "    0.5 = 60\n",
      "    0.75 = 60\n",
      "    1.0 = 60\n",
      "    2.5 = 60\n",
      "    5.0 = 60\n",
      "    7.5 = 60\n",
      "    10.0 = 60\n",
      "    20.0 = 60\n",
      "    40.0 = 60\n",
      "    80.0 = 60\n",
      "    +Inf = 60\n",
      "vllm:e2e_request_latency_seconds (histogram)\n",
      "    sum = 0.8255488872528076\n",
      "    count = 4\n",
      "    0.3 = 4\n",
      "    0.5 = 4\n",
      "    0.8 = 4\n",
      "    1.0 = 4\n",
      "    1.5 = 4\n",
      "    2.0 = 4\n",
      "    2.5 = 4\n",
      "    5.0 = 4\n",
      "    10.0 = 4\n",
      "    15.0 = 4\n",
      "    20.0 = 4\n",
      "    30.0 = 4\n",
      "    40.0 = 4\n",
      "    50.0 = 4\n",
      "    60.0 = 4\n",
      "    120.0 = 4\n",
      "    240.0 = 4\n",
      "    480.0 = 4\n",
      "    960.0 = 4\n",
      "    1920.0 = 4\n",
      "    7680.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:request_queue_time_seconds (histogram)\n",
      "    sum = 5.251800757832825e-05\n",
      "    count = 4\n",
      "    0.3 = 4\n",
      "    0.5 = 4\n",
      "    0.8 = 4\n",
      "    1.0 = 4\n",
      "    1.5 = 4\n",
      "    2.0 = 4\n",
      "    2.5 = 4\n",
      "    5.0 = 4\n",
      "    10.0 = 4\n",
      "    15.0 = 4\n",
      "    20.0 = 4\n",
      "    30.0 = 4\n",
      "    40.0 = 4\n",
      "    50.0 = 4\n",
      "    60.0 = 4\n",
      "    120.0 = 4\n",
      "    240.0 = 4\n",
      "    480.0 = 4\n",
      "    960.0 = 4\n",
      "    1920.0 = 4\n",
      "    7680.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:request_inference_time_seconds (histogram)\n",
      "    sum = 0.7272132380167022\n",
      "    count = 4\n",
      "    0.3 = 4\n",
      "    0.5 = 4\n",
      "    0.8 = 4\n",
      "    1.0 = 4\n",
      "    1.5 = 4\n",
      "    2.0 = 4\n",
      "    2.5 = 4\n",
      "    5.0 = 4\n",
      "    10.0 = 4\n",
      "    15.0 = 4\n",
      "    20.0 = 4\n",
      "    30.0 = 4\n",
      "    40.0 = 4\n",
      "    50.0 = 4\n",
      "    60.0 = 4\n",
      "    120.0 = 4\n",
      "    240.0 = 4\n",
      "    480.0 = 4\n",
      "    960.0 = 4\n",
      "    1920.0 = 4\n",
      "    7680.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:request_prefill_time_seconds (histogram)\n",
      "    sum = 0.08100005402229726\n",
      "    count = 4\n",
      "    0.3 = 4\n",
      "    0.5 = 4\n",
      "    0.8 = 4\n",
      "    1.0 = 4\n",
      "    1.5 = 4\n",
      "    2.0 = 4\n",
      "    2.5 = 4\n",
      "    5.0 = 4\n",
      "    10.0 = 4\n",
      "    15.0 = 4\n",
      "    20.0 = 4\n",
      "    30.0 = 4\n",
      "    40.0 = 4\n",
      "    50.0 = 4\n",
      "    60.0 = 4\n",
      "    120.0 = 4\n",
      "    240.0 = 4\n",
      "    480.0 = 4\n",
      "    960.0 = 4\n",
      "    1920.0 = 4\n",
      "    7680.0 = 4\n",
      "    +Inf = 4\n",
      "vllm:request_decode_time_seconds (histogram)\n",
      "    sum = 0.646213183994405\n",
      "    count = 4\n",
      "    0.3 = 4\n",
      "    0.5 = 4\n",
      "    0.8 = 4\n",
      "    1.0 = 4\n",
      "    1.5 = 4\n",
      "    2.0 = 4\n",
      "    2.5 = 4\n",
      "    5.0 = 4\n",
      "    10.0 = 4\n",
      "    15.0 = 4\n",
      "    20.0 = 4\n",
      "    30.0 = 4\n",
      "    40.0 = 4\n",
      "    50.0 = 4\n",
      "    60.0 = 4\n",
      "    120.0 = 4\n",
      "    240.0 = 4\n",
      "    480.0 = 4\n",
      "    960.0 = 4\n",
      "    1920.0 = 4\n",
      "    7680.0 = 4\n",
      "    +Inf = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W926 20:18:27.938780685 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78464705-5a65-4c5d-b51a-496db6d0c34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
