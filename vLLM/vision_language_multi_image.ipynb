{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3830a67-4af4-415e-a5f1-e7e6f78d4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import asdict\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from PIL.Image import Image\n",
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "\n",
    "from vllm import LLM, EngineArgs, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm.multimodal.utils import fetch_image\n",
    "from vllm.utils import FlexibleArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4061e5f9-e645-4ed5-8ca9-ac49055a4283",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRequestData(NamedTuple):\n",
    "    engine_args: EngineArgs\n",
    "    prompt: str\n",
    "    image_data: list[Image]\n",
    "    stop_token_ids: Optional[list[int]] = None\n",
    "    chat_template: Optional[str] = None\n",
    "    lora_requests: Optional[list[LoRARequest]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ad4b5-56bd-433c-9109-910ded9aba30",
   "metadata": {},
   "source": [
    "### Aria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf4111f-d00e-46de-8ed2-d63ed5ba7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aria(question: str, image_urls: list[str]) -> ModelRequestData:\n",
    "    model_name = \"rhymes-ai/Aria\"\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        tokenizer_mode=\"slow\",\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        limit_mm_per_prompt={\"image\": len(image_urls)},\n",
    "    )\n",
    "    placeholders = \"<fim_prefix><|img|><fim_suffix>\\n\" * len(image_urls)\n",
    "    prompt = (\n",
    "        f\"<|im_start|>user\\n{placeholders}{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    )\n",
    "    stop_token_ids = [93532, 93653, 944, 93421, 1019, 93653, 93519]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompt=prompt,\n",
    "        stop_token_ids=stop_token_ids,\n",
    "        image_data=[fetch_image(url) for url in image_urls],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d301c793-733b-4640-8778-4c2777b4eaec",
   "metadata": {},
   "source": [
    "### Gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2de79f-834d-440a-a222-d59331a8076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gemma3(question: str, image_urls: list[str]) -> ModelRequestData:\n",
    "    model_name = \"google/gemma-3-4b-it\"\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        max_model_len=8192,\n",
    "        max_num_seqs=2,\n",
    "        limit_mm_per_prompt={\"image\": len(image_urls)},\n",
    "    )\n",
    "\n",
    "    placeholders = [{\"type\": \"image\", \"image\": url} for url in image_urls]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *placeholders,\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompt=prompt,\n",
    "        image_data=[fetch_image(url) for url in image_urls],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1430e-640a-4fe3-b745-a0fb9edcd6f9",
   "metadata": {},
   "source": [
    "### H2OVL-Mississippi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4d2ba3-9408-4978-b7e7-845f49d85c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h2ovl(question: str, image_urls: list[str]) -> ModelRequestData:\n",
    "    model_name = \"h2oai/h2ovl-mississippi-800m\"\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        trust_remote_code=True,\n",
    "        max_model_len=8192,\n",
    "        limit_mm_per_prompt={\"image\": len(image_urls)},\n",
    "        mm_processor_kwargs={\"max_dynamic_patch\": 4},\n",
    "    )\n",
    "\n",
    "    placeholders = \"\\n\".join(\n",
    "        f\"Image-{i}: <image>\\n\" for i, _ in enumerate(image_urls, start=1)\n",
    "    )\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{placeholders}\\n{question}\"}]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Stop tokens for H2OVL-Mississippi\n",
    "    # https://huggingface.co/h2oai/h2ovl-mississippi-800m\n",
    "    stop_token_ids = [tokenizer.eos_token_id]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompt=prompt,\n",
    "        stop_token_ids=stop_token_ids,\n",
    "        image_data=[fetch_image(url) for url in image_urls],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec90893-a41d-4f91-86a4-5110839f94d0",
   "metadata": {},
   "source": [
    "### LLaVA-1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfc9e35-7ece-4157-afac-e4eaab5c1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llava(question: str, image_urls: list[str]) -> ModelRequestData:\n",
    "    # NOTE: CAUTION! Original Llava models wasn't really trained on multi-image inputs,\n",
    "    # it will generate poor response for multi-image inputs!\n",
    "    model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        max_num_seqs=16,\n",
    "        limit_mm_per_prompt={\"image\": len(image_urls)},\n",
    "    )\n",
    "\n",
    "    placeholders = [{\"type\": \"image\", \"image\": url} for url in image_urls]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *placeholders,\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompt=prompt,\n",
    "        image_data=[fetch_image(url) for url in image_urls],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bc4de-04f4-485f-a3cd-18ab8bd24525",
   "metadata": {},
   "source": [
    "### LLaVA-OneVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "564da0d9-88a6-4069-818c-3f994961bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llava_onevision(question: str, image_urls: list[str]) -> ModelRequestData:\n",
    "    model_name = \"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        max_model_len=16384,\n",
    "        max_num_seqs=16,\n",
    "        limit_mm_per_prompt={\"image\": len(image_urls)},\n",
    "    )\n",
    "\n",
    "    placeholders = [{\"type\": \"image\", \"image\": url} for url in image_urls]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *placeholders,\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompt=prompt,\n",
    "        image_data=[fetch_image(url) for url in image_urls],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb724976-0e55-46c5-9162-66e0a57ee95b",
   "metadata": {},
   "source": [
    "## Model map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1c90d16-9742-4bb2-9cce-e18c15177a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_example_map = {\n",
    "    \"aria\": load_aria,\n",
    "    \"gemma3\": load_gemma3,\n",
    "    \"h2ovl_chat\": load_h2ovl,\n",
    "    \"llava\": load_llava,\n",
    "    \"llava-onevision\": load_llava_onevision,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783509f8-3ea1-4d8c-8ded-caee29fe2a3f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f8d1f6-77d8-4a2a-afc9-d452837265ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is the content of each image?\"\n",
    "IMAGE_URLS = [\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/d/da/2015_Kaczka_krzy%C5%BCowka_w_wodzie_%28samiec%29.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/7/77/002_The_lion_king_Snyggve_in_the_Serengeti_National_Park_Photo_by_Giles_Laurent.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/2/26/Ultramarine_Flycatcher_%28Ficedula_superciliaris%29_Naggar%2C_Himachal_Pradesh%2C_2013_%28cropped%29.JPG\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/Anim1754_-_Flickr_-_NOAA_Photo_Library_%281%29.jpg/2560px-Anim1754_-_Flickr_-_NOAA_Photo_Library_%281%29.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/d/d4/Starfish%2C_Caswell_Bay_-_geograph.org.uk_-_409413.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/6/69/Grapevinesnail_01.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Texas_invasive_Musk_Thistle_1.jpg/1920px-Texas_invasive_Musk_Thistle_1.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Huskiesatrest.jpg/2880px-Huskiesatrest.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg/1920px-Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/3/30/George_the_amazing_guinea_pig.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Oryctolagus_cuniculus_Rcdo.jpg/1920px-Oryctolagus_cuniculus_Rcdo.jpg\",\n",
    "    \"https://upload.wikimedia.org/wikipedia/commons/9/98/Horse-and-pony.jpg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a10067c-9d36-4705-af15-9d1bdba12de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(model, question: str, image_urls: list[str], seed: Optional[int]):\n",
    "    req_data = model_example_map[model](question, image_urls)\n",
    "\n",
    "    engine_args = asdict(req_data.engine_args) | {\"seed\": seed}\n",
    "    llm = LLM(**engine_args)\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0, max_tokens=256, stop_token_ids=req_data.stop_token_ids\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        {\n",
    "            \"prompt\": req_data.prompt,\n",
    "            \"multi_modal_data\": {\"image\": req_data.image_data},\n",
    "        },\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=req_data.lora_requests,\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    for o in outputs:\n",
    "        generated_text = o.outputs[0].text\n",
    "        print(generated_text)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e94f78f-a8bf-44dd-8cba-36a9bb743b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chat(model: str, question: str, image_urls: list[str], seed: Optional[int]):\n",
    "    req_data = model_example_map[model](question, image_urls)\n",
    "\n",
    "    # Disable other modalities to save memory\n",
    "    default_limits = {\"image\": 0, \"video\": 0, \"audio\": 0}\n",
    "    req_data.engine_args.limit_mm_per_prompt = default_limits | dict(\n",
    "        req_data.engine_args.limit_mm_per_prompt or {}\n",
    "    )\n",
    "\n",
    "    engine_args = asdict(req_data.engine_args) | {\"seed\": seed}\n",
    "    llm = LLM(**engine_args)\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0, max_tokens=256, stop_token_ids=req_data.stop_token_ids\n",
    "    )\n",
    "    outputs = llm.chat(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": question,\n",
    "                    },\n",
    "                    *(\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": image_url},\n",
    "                        }\n",
    "                        for image_url in image_urls\n",
    "                    ),\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        sampling_params=sampling_params,\n",
    "        chat_template=req_data.chat_template,\n",
    "        lora_request=req_data.lora_requests,\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    for o in outputs:\n",
    "        generated_text = o.outputs[0].text\n",
    "        print(generated_text)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2edd9a3-8227-4c18-853b-2bb303b6f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(method: str, model: str, seed: int, question: str, image_urls: list[str]):\n",
    "    if method == \"generate\":\n",
    "        run_generate(model, question, image_urls, seed)\n",
    "    elif method == \"chat\":\n",
    "        run_chat(model, question, image_urls, seed)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d8a6899-1373-4cee-b888-5d53a41aa2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 19:46:14 [config.py:853] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 09-19 19:46:14 [config.py:1467] Using max model len 65536\n",
      "INFO 09-19 19:46:14 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-19 19:46:14 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "WARNING 09-19 19:46:15 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 09-19 19:46:18 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-19 19:46:26 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-19 19:46:26 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='rhymes-ai/Aria', speculative_config=None, tokenizer='rhymes-ai/Aria', skip_tokenizer_init=False, tokenizer_mode=slow, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=rhymes-ai/Aria, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-19 19:46:27 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x78131f023dd0>\n",
      "INFO 09-19 19:46:27 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 09-19 19:46:27 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 09-19 19:46:29 [gpu_model_runner.py:1751] Starting to load model rhymes-ai/Aria...\n",
      "INFO 09-19 19:46:29 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-19 19:46:29 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-19 19:46:29 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-19 19:46:29 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 1/12 [00:01<00:13,  1.19s/it]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:02<00:10,  1.04s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 3/12 [00:03<00:09,  1.03s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 4/12 [00:04<00:07,  1.02it/s]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 5/12 [00:05<00:06,  1.00it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 6/12 [00:05<00:05,  1.16it/s]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:06<00:04,  1.16it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 8/12 [00:07<00:04,  1.01s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 9/12 [00:08<00:02,  1.02it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 10/12 [00:09<00:01,  1.01it/s]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 11/12 [00:10<00:01,  1.00s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:11<00:00,  1.03it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:11<00:00,  1.03it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 19:46:41 [default_loader.py:272] Loading weights took 11.76 seconds\n",
      "INFO 09-19 19:46:41 [gpu_model_runner.py:1782] Model loading took 49.8945 GiB and 12.099335 seconds\n",
      "INFO 09-19 19:46:41 [gpu_model_runner.py:2221] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "INFO 09-19 19:47:06 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/7382194048/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-19 19:47:06 [backends.py:520] Dynamo bytecode transform time: 4.13 s\n",
      "INFO 09-19 19:47:08 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 0.367 s\n",
      "INFO 09-19 19:47:08 [monitor.py:34] torch.compile takes 4.13 s in total\n",
      "WARNING 09-19 19:47:08 [fused_moe.py:683] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=1664,device_name=AMD_Instinct_MI300X.json\n",
      "INFO 09-19 19:47:09 [gpu_worker.py:232] Available KV cache memory: 109.67 GiB\n",
      "INFO 09-19 19:47:09 [kv_cache_utils.py:716] GPU KV cache size: 410,704 tokens\n",
      "INFO 09-19 19:47:09 [kv_cache_utils.py:720] Maximum concurrency for 65,536 tokens per request: 6.27x\n",
      "INFO 09-19 19:47:09 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:10<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 19:47:19 [gpu_model_runner.py:2306] Graph capturing finished in 10 secs, took 0.37 GiB\n",
      "INFO 09-19 19:47:19 [core.py:172] init engine (profile, create kv cache, warmup model) took 38.32 seconds\n",
      "WARNING 09-19 19:47:20 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38c5c60e1de4f1cb20ef4e32619c3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e241962060943f1a0a10ba15197b0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "  1 . A duck in water  2 . A lion in grass  3 . A bird on a tree branch  4 . A whale in water  5 . A starfish on the beach  6 . A snail on a white background  7 . A flower with a bee  8 . Two dogs in the snow  9 . A cat on the ground  1 0 . A guinea pig on a table  1 1 . A rabbit in the forest  1 2 . A horse and a pony in a field\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W919 19:47:23.583689694 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "main(\"generate\", \"aria\", None, QUESTION, IMAGE_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f69a001-0121-4c3b-8ed4-86a32619f46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 19:50:51 [config.py:853] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 09-19 19:50:51 [config.py:1467] Using max model len 65536\n",
      "INFO 09-19 19:50:51 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-19 19:50:51 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "WARNING 09-19 19:50:52 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 09-19 19:50:54 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-19 19:51:03 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-19 19:51:03 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='rhymes-ai/Aria', speculative_config=None, tokenizer='rhymes-ai/Aria', skip_tokenizer_init=False, tokenizer_mode=slow, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=rhymes-ai/Aria, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-19 19:51:04 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7deb63fe70b0>\n",
      "INFO 09-19 19:51:04 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 09-19 19:51:04 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 09-19 19:51:05 [gpu_model_runner.py:1751] Starting to load model rhymes-ai/Aria...\n",
      "INFO 09-19 19:51:06 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-19 19:51:06 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-19 19:51:06 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-19 19:51:06 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 1/12 [00:01<00:12,  1.17s/it]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:02<00:09,  1.00it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 3/12 [00:03<00:08,  1.01it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 4/12 [00:03<00:07,  1.07it/s]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 5/12 [00:04<00:06,  1.05it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 6/12 [00:05<00:05,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:06<00:04,  1.12it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 8/12 [00:07<00:04,  1.01s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 9/12 [00:08<00:02,  1.04it/s]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 10/12 [00:09<00:01,  1.03it/s]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 11/12 [00:10<00:00,  1.03it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:11<00:00,  1.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:11<00:00,  1.05it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 19:51:17 [default_loader.py:272] Loading weights took 11.51 seconds\n",
      "INFO 09-19 19:51:18 [gpu_model_runner.py:1782] Model loading took 49.8945 GiB and 11.905837 seconds\n",
      "INFO 09-19 19:51:18 [gpu_model_runner.py:2221] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "INFO 09-19 19:51:42 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/7382194048/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-19 19:51:42 [backends.py:520] Dynamo bytecode transform time: 4.18 s\n",
      "INFO 09-19 19:51:44 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 0.363 s\n",
      "INFO 09-19 19:51:45 [monitor.py:34] torch.compile takes 4.18 s in total\n",
      "WARNING 09-19 19:51:45 [fused_moe.py:683] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=1664,device_name=AMD_Instinct_MI300X.json\n",
      "INFO 09-19 19:51:46 [gpu_worker.py:232] Available KV cache memory: 109.67 GiB\n",
      "INFO 09-19 19:51:46 [kv_cache_utils.py:716] GPU KV cache size: 410,704 tokens\n",
      "INFO 09-19 19:51:46 [kv_cache_utils.py:720] Maximum concurrency for 65,536 tokens per request: 6.27x\n",
      "INFO 09-19 19:51:46 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:10<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 19:51:56 [gpu_model_runner.py:2306] Graph capturing finished in 10 secs, took 0.37 GiB\n",
      "INFO 09-19 19:51:56 [core.py:172] init engine (profile, create kv cache, warmup model) took 38.54 seconds\n",
      "WARNING 09-19 19:51:58 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 09-19 19:51:59 [chat_utils.py:421] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\n",
      "WARNING 09-19 19:52:00 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8b698f7efe43b89d00740cc9835d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f14774550d437b915b5eef3579ff71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      " 1 . The first image shows a mallard duck swimming in a body of water. \n",
      " 2 . The second image features a majestic lion sitting in a grassy field. \n",
      " 3 . The third image captures a blue and white bird perched on a branch, holding an insect in its beak. \n",
      " 4 . The fourth image displays a whale swimming in the ocean, viewed from above. \n",
      " 5 . The fifth image shows a starfish lying on a sandy beach. \n",
      " 6 . The sixth image depicts a snail crawling on a white surface. \n",
      " 7 . The seventh image shows a purple thistle flower with a bee on it. \n",
      " 8 . The eighth image captures two sled dogs standing in the snow. \n",
      " 9 . The ninth image shows a ginger and white cat sitting on a bed of fallen leaves. \n",
      " 1 0 . The tenth image shows a black and white guinea pig sitting on a wet surface. \n",
      " 1 1 . The eleventh image shows a brown rabbit sitting on the ground in a forest. \n",
      " 1 2 . The twelfth image shows a horse and a pony running in a field.<\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W919 19:52:06.406343150 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "main(\"chat\", \"aria\", None, QUESTION, IMAGE_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309665b-5cf0-485d-beb0-eb3a4641028e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
