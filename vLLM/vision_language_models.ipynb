{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f089ac3-ec62-4326-bce5-d2f1c968c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import asdict\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from vllm import LLM, EngineArgs, SamplingParams\n",
    "from vllm.assets.image import ImageAsset\n",
    "from vllm.assets.video import VideoAsset\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm.multimodal.image import convert_image_mode\n",
    "from vllm.utils import FlexibleArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4b877a-fd8d-4f9c-8cac-70db9450f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRequestData(NamedTuple):\n",
    "    engine_args: EngineArgs\n",
    "    prompts: list[str]\n",
    "    stop_token_ids: Optional[list[int]] = None\n",
    "    lora_requests: Optional[list[LoRARequest]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2b149-2476-40cc-a176-16fbc8114eb2",
   "metadata": {},
   "source": [
    "### Aria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127a56e0-b62f-4c34-9933-0da186410634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_aria(questions: list[str], modality: str) -> ModelRequestData:\n",
    "    assert modality == \"image\"\n",
    "    model_name = \"rhymes-ai/Aria\"\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        max_model_len=4096,\n",
    "        max_num_seqs=2,\n",
    "        dtype=\"bfloat16\",\n",
    "        limit_mm_per_prompt={modality: 1},\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        (\n",
    "            f\"<|im_start|>user\\n<fim_prefix><|img|><fim_suffix>{question}\"\n",
    "            \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        for question in questions\n",
    "    ]\n",
    "\n",
    "    stop_token_ids = [93532, 93653, 944, 93421, 1019, 93653, 93519]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "        stop_token_ids=stop_token_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b5a584-1bab-48f3-9c80-b4a4f80e8de3",
   "metadata": {},
   "source": [
    "### BLIP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b32d98b-3f13-4e44-b1c1-2d675403a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_blip2(questons: list[str], modality: str) -> ModelRequestData:\n",
    "    assert modality == \"image\"\n",
    "    model_name=\"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        limit_mm_per_prompt={modality: 1}\n",
    "    )\n",
    "\n",
    "    prompts = [f\"Question: {question} Answer:\" for question in questions]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98975b-ac57-4f16-a5af-a9c9f381e907",
   "metadata": {},
   "source": [
    "### Ernie4.5-VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a53182c-da35-41d4-a48e-5ad73ef464f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ernie45_vl(questions: list[str], modality: str) -> ModelRequestData:\n",
    "    model_name = \"baidu/ERNIE-4.5-VL-28B-A3B-PT\"\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        max_model_len=4096,\n",
    "        max_num_seqs=5,\n",
    "        limit_mm_per_prompt={modality: 1},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if modality == \"image\":\n",
    "        placeholder = \"Picture 1:<|IMAGE_START|><|image@placeholder|><|IMAGE_END|>\"\n",
    "    elif modality == \"video\":\n",
    "        placeholder = \"Video 1:<|VIDEO_START|><|video@placeholder|><|VIDEO_END|>\"\n",
    "\n",
    "    prompts = [\n",
    "        (\n",
    "            f\"<|begin_of_sentence|>User: {question}{placeholder}\\n\"\n",
    "            \"Assistant: <think></think>\"\n",
    "        )\n",
    "        for question in questions\n",
    "    ]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6cf13-8b7f-4f9f-bb9f-65fcef6900b0",
   "metadata": {},
   "source": [
    "### Gemma3N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ec1ab20-e84f-48fd-bb66-285f8bf173da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gemma3n(questions: list[str], modality: str) -> ModelRequestData:\n",
    "    assert modality == \"image\"\n",
    "    model_name = \"google/gemma-3n-E2B-it\"\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        max_model_len=2048,\n",
    "        max_num_seqs=2,\n",
    "        limit_mm_per_prompt={modality: 1},\n",
    "        enforce_eager=True,\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        (\n",
    "            \"<start_of_turn>user\\n\"\n",
    "            f\"<image_soft_token>{question}<end_of_turn>\\n\"\n",
    "            \"<start_of_turn>model\\n\"\n",
    "        )\n",
    "        for question in questions\n",
    "    ]\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff204224-1c6c-4164-b9c3-4a3fa7a02373",
   "metadata": {},
   "source": [
    "### H2OVL-Mississippi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde0d485-21fb-4f5a-abb3-bca95db63894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_h2ovl(questions: list[str], modality: str) -> ModelRequestData:\n",
    "    assert modality == \"image\"\n",
    "    model_name = \"h2oai/h2ovl-mississippi-800m\"\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        trust_remote_code=True,\n",
    "        max_model_len=8192,\n",
    "        limit_mm_per_prompt={modality: 1},\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    messages = [\n",
    "        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n",
    "    ]\n",
    "    prompts = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    stop_token_ids = [tokenizer.eos_token_id]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "        stop_token_ids=stop_token_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3cb016-6a15-4ad3-be2e-bd497b3536ff",
   "metadata": {},
   "source": [
    "### LLaVA-1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c832e4c3-9717-47b2-8491-31760c14cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llava(questions: list[str], modality: str) -> ModelRequestData:\n",
    "    assert modality == \"image\"\n",
    "    modal_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "    prompts = [f\"USER: <image>\\n{question}\\nASSISTANT:\" for question in questions]\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=modal_name,\n",
    "        max_model_len=4096,\n",
    "        limit_mm_per_prompt={modality: 1},\n",
    "    )\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526c8ac-4b63-49ec-bdda-96f426f36c22",
   "metadata": {},
   "source": [
    "### LLaVA-OneVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58782f46-1cf9-4bfc-afbb-2b5e01337b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llava_onevision(questions: list[str], modality: str) -> ModelRequestData:\n",
    "    model_name=\"llava-hf/llava-onevision-qwen2-7b-ov-hf\"\n",
    "    \n",
    "    if modality == \"video\":\n",
    "        prompts = [\n",
    "            f\"<|im_start|>user <video>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n",
    "            for question in questions\n",
    "        ]\n",
    "\n",
    "    elif modality == \"image\":\n",
    "        prompts = [\n",
    "            f\"<|im_start|>user <image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n",
    "            for question in questions\n",
    "        ]\n",
    "\n",
    "    engine_args = EngineArgs(\n",
    "        model=model_name,\n",
    "        max_model_len=16384,\n",
    "        limit_mm_per_prompt={modality: 1},\n",
    "    )\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e65b50-a09f-40f3-a9f6-ed122d45e708",
   "metadata": {},
   "source": [
    "## Model map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30e0ecf9-9b9d-488c-bf46-d73d9c0e3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_example_map = {\n",
    "    \"aria\": run_aria,\n",
    "    \"blip-2\": run_blip2,\n",
    "    \"ernie45_vl\": run_ernie45_vl,\n",
    "    \"gemma3n\": run_gemma3n,\n",
    "    \"h2ovl_chat\": run_h2ovl,\n",
    "    \"llava\": run_llava,\n",
    "    \"llava-onevision\": run_llava_onevision,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58590f2-369b-420e-b247-df3993d3ec69",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "494896be-d15e-4cd5-a966-cfedf8eecd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_modal_input(args):\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"data\": image or video,\n",
    "        \"question\": question,\n",
    "    }\n",
    "    \"\"\"\n",
    "    if args.modality == \"image\":\n",
    "        # Input image and question\n",
    "        image = convert_image_mode(ImageAsset(\"cherry_blossom\").pil_image, \"RGB\")\n",
    "        img_questions = [\n",
    "            \"What is the content of this image?\",\n",
    "            \"Describe the content of this image in detail.\",\n",
    "            \"What's in the image?\",\n",
    "            \"Where is this image taken?\",\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"data\": image,\n",
    "            \"questions\": img_questions,\n",
    "        }\n",
    "\n",
    "    if args.modality == \"video\":\n",
    "        # Input video and question\n",
    "        video = VideoAsset(name=\"baby_reading\", num_frames=args.num_frames).np_ndarrays\n",
    "        # metadata = VideoAsset(name=\"baby_reading\", num_frames=args.num_frames).metadata\n",
    "        vid_questions = [\"Why is this video funny?\"]\n",
    "\n",
    "        return {\n",
    "            # \"data\": [(video, metadata)] if args.model_type == \"glm4_1v\" else video,\n",
    "            \"data\": video,\n",
    "            \"questions\": vid_questions,\n",
    "        }\n",
    "\n",
    "    msg = f\"Modality {args.modality} is not supported.\"\n",
    "    raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f085cbb-0643-4ac4-b6a7-14702f7ec900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_image_repeat(\n",
    "    image_repeat_prob, num_prompts, data, prompts: list[str], modality\n",
    "):\n",
    "    \"\"\"Repeats images with provided probability of \"image_repeat_prob\".\n",
    "    Used to simulate hit/miss for the MM preprocessor cache.\n",
    "    \"\"\"\n",
    "    assert image_repeat_prob <= 1.0 and image_repeat_prob >= 0\n",
    "    no_yes = [0, 1]\n",
    "    probs = [1.0 - image_repeat_prob, image_repeat_prob]\n",
    "\n",
    "    inputs = []\n",
    "    cur_image = data\n",
    "    for i in range(num_prompts):\n",
    "        if image_repeat_prob is not None:\n",
    "            res = random.choices(no_yes, probs)[0]\n",
    "            if res == 0:\n",
    "                # No repeat => Modify one pixel\n",
    "                cur_image = cur_image.copy()\n",
    "                new_val = (i // 256 // 256, i // 256, i % 256)\n",
    "                cur_image.putpixel((0, 0), new_val)\n",
    "\n",
    "        inputs.append(\n",
    "            {\n",
    "                \"prompt\": prompts[i % len(prompts)],\n",
    "                \"multi_modal_data\": {modality: cur_image},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e58fcfa9-bb49-455b-93a3-6362bfd992bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def time_counter(enable: bool):\n",
    "    if enable:\n",
    "        import time\n",
    "\n",
    "        start_time = time.time()\n",
    "        yield\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"-\" * 50)\n",
    "        print(\"-- generate time = {}\".format(elapsed_time))\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38462404-6994-4d02-b1a1-137f1b82e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    model = args.model_type\n",
    "    if model not in model_example_map:\n",
    "        raise ValueError(f\"Model type {model} is not supported.\")\n",
    "\n",
    "    modality = args.modality\n",
    "    mm_input = get_multi_modal_input(args)\n",
    "    data = mm_input[\"data\"]\n",
    "    questions = mm_input[\"questions\"]\n",
    "    req_data = model_example_map[model](questions, modality)\n",
    "\n",
    "    default_limits = {\"image\": 0, \"video\": 0, \"audio\": 0}\n",
    "    req_data.engine_args.limit_mm_per_prompt = default_limits | dict(\n",
    "        req_data.engine_args.limit_mm_per_prompt or {}\n",
    "    )\n",
    "    engine_args = asdict(req_data.engine_args) | {\n",
    "        \"seed\": args.seed,\n",
    "        # \"mm_processor_cache_gb\": 0 if args.disable_mm_processor_cache else 4,\n",
    "    }\n",
    "\n",
    "    llm = LLM(**engine_args)\n",
    "\n",
    "    prompts = (\n",
    "        req_data.prompts\n",
    "        if args.use_different_prompt_per_request\n",
    "        else [req_data.prompts[0]]\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.2, max_tokens=64, stop_token_ids=req_data.stop_token_ids\n",
    "    )\n",
    "\n",
    "    assert args.num_prompts > 0\n",
    "    if args.num_prompts == 1:\n",
    "        inputs = {\n",
    "            \"prompt\": prompts[0],\n",
    "            \"multi_modal_data\": {modality: data},\n",
    "        }\n",
    "    else:\n",
    "        if args.image_repeat_prob is not None:\n",
    "            inputs = apply_image_repeat(\n",
    "                args.image_repeat_prob, args.num_prompts, data, prompts, modality\n",
    "            )\n",
    "        else:\n",
    "            inputs = [\n",
    "                {\n",
    "                    \"prompt\": prompts[i % len(prompts)],\n",
    "                    \"multi_modal_data\": {modality: data},\n",
    "                }\n",
    "                for i in range(args.num_prompts)\n",
    "            ]\n",
    "\n",
    "    lora_request = (\n",
    "        req_data.lora_requests * args.num_prompts if req_data.lora_requests else None\n",
    "    )\n",
    "\n",
    "    with time_counter(args.time_generate):\n",
    "        outputs = llm.generate(\n",
    "            inputs,\n",
    "            sampling_params=sampling_params,\n",
    "            lora_request=lora_request,\n",
    "        )\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    for o in outputs:\n",
    "        generated_text = o.outputs[0].text\n",
    "        print(generated_text)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57d906d4-d768-4faf-a30e-4d1e2beaa1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee6111-e031-46af-a038-a4266a91ffe7",
   "metadata": {},
   "source": [
    "#### Image example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c65ff46b-bf45-4610-879e-4d5deff66178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71ab2195e4447f792e6cc57d5f8927f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3b05551d5248a7aa6b4765c08dbb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:00:38 [config.py:853] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e9a10cb72e400ea856520ffbdbbdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:00:38 [config.py:1467] Using max model len 4096\n",
      "INFO 09-12 20:00:46 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-12 20:00:46 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e996e0d6bd44d9924e458388ec6735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32089f0260b041fc80bb00568f930403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b587228c7bc4b228c748dc68db7fa53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2522a347c74d7ea6c3da78d5e09ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4132e7f36048bda6875d987a179659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5907588e144712941633bfa815fc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-12 20:00:48 [utils.py:2613] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 09-12 20:00:50 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-12 20:00:59 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-12 20:00:59 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='llava-hf/llava-1.5-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-1.5-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llava-hf/llava-1.5-7b-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-12 20:00:59 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x707d36eeef60>\n",
      "INFO 09-12 20:01:00 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:01:01 [gpu_model_runner.py:1751] Starting to load model llava-hf/llava-1.5-7b-hf...\n",
      "INFO 09-12 20:01:01 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-12 20:01:01 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-12 20:01:01 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-12 20:01:01 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-12 20:01:12 [weight_utils.py:308] Time spent downloading weights for llava-hf/llava-1.5-7b-hf: 10.752270 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:02,  1.40s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:03<00:01,  1.63s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:05<00:00,  1.73s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:05<00:00,  1.68s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:01:17 [default_loader.py:272] Loading weights took 5.16 seconds\n",
      "INFO 09-12 20:01:17 [gpu_model_runner.py:1782] Model loading took 13.2012 GiB and 16.181506 seconds\n",
      "INFO 09-12 20:01:17 [gpu_model_runner.py:2221] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 29 image items of the maximum feature size.\n",
      "INFO 09-12 20:01:41 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/0b9de6558d/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-12 20:01:41 [backends.py:520] Dynamo bytecode transform time: 4.72 s\n",
      "INFO 09-12 20:01:58 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 09-12 20:01:58 [backends.py:193] Compiling a graph for general shape takes 14.12 s\n",
      "INFO 09-12 20:02:00 [monitor.py:34] torch.compile takes 18.85 s in total\n",
      "INFO 09-12 20:02:02 [gpu_worker.py:232] Available KV cache memory: 156.20 GiB\n",
      "INFO 09-12 20:02:02 [kv_cache_utils.py:716] GPU KV cache size: 319,888 tokens\n",
      "INFO 09-12 20:02:02 [kv_cache_utils.py:720] Maximum concurrency for 4,096 tokens per request: 78.10x\n",
      "INFO 09-12 20:02:02 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:38<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:02:41 [gpu_model_runner.py:2306] Graph capturing finished in 39 secs, took 0.56 GiB\n",
      "INFO 09-12 20:02:41 [core.py:172] init engine (profile, create kv cache, warmup model) took 83.53 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939f5aef32f741cc9ff606ce9b1eb9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfc6ae20ca34aebab36f7eba57360c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-- generate time = 1.930650234222412\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      " The image features a tall tower with a spire, surrounded by a beautiful flowering tree. The tree is filled with pink flowers, creating a vibrant and lively atmosphere. The tower stands tall in the background, with the tree's branches extending towards it. The combination of the tower and the flow\n",
      "--------------------------------------------------\n",
      " The image features a tall tower with a spire, surrounded by a beautiful cherry blossom tree. The tree is filled with pink flowers, creating a picturesque scene. The tower stands tall in the background, with the blossoming tree in the foreground. The combination of the tower and the v\n",
      "--------------------------------------------------\n",
      " The image features a tall tower with a spire, surrounded by a beautiful cherry blossom tree. The tree is filled with pink flowers, creating a picturesque scene. The tower stands tall in the background, with the blossoming tree in the foreground. The combination of the tower and the tree\n",
      "--------------------------------------------------\n",
      " The image features a tall tower with a spire, surrounded by a beautiful cherry blossom tree. The tree is filled with pink flowers, creating a picturesque scene. The tower stands tall in the background, with the blossoming tree in the foreground. The combination of the tower and the tree\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W912 20:02:43.982156394 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "args = Args(\n",
    "    model_type=\"llava\",\n",
    "    num_prompts=4,\n",
    "    modality=\"image\",\n",
    "    num_frames=16,\n",
    "    seed=None,\n",
    "    image_repeat_prob=None,\n",
    "    disable_mm_processor_cache=False,\n",
    "    time_generate=True,\n",
    "    use_different_prompt_per_request=False\n",
    ")\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4cf81-45d8-47d6-9c3f-67935a5e4d01",
   "metadata": {},
   "source": [
    "#### Video example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8682db4-8ccf-4a12-9b09-c475c0f790de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbd4c0610754918b66194faa2699369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97cd13f5d134c6a853542591f886ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:07:41 [config.py:853] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f737c06eef74a078ce7c0925ac8f4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:07:41 [config.py:1467] Using max model len 16384\n",
      "INFO 09-12 20:07:41 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-12 20:07:41 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a80c2cdd4f64ecca89f5dc9cea1a98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159b851cab124ad5b860ebe9d3f56d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0014f8f736e4416a51ec62fed72aec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282b821cd9494890ab6a3f8577143748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f74996c6b9f43d58ecfb1c39a5cac6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7da40adf6e04d5c9212e6a12b8a52d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/126 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:07:45 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-12 20:07:54 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-12 20:07:54 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='llava-hf/llava-onevision-qwen2-7b-ov-hf', speculative_config=None, tokenizer='llava-hf/llava-onevision-qwen2-7b-ov-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llava-hf/llava-onevision-qwen2-7b-ov-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-12 20:07:54 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7addd9f9d160>\n",
      "INFO 09-12 20:07:54 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:07:56 [gpu_model_runner.py:1751] Starting to load model llava-hf/llava-onevision-qwen2-7b-ov-hf...\n",
      "INFO 09-12 20:07:56 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-12 20:07:56 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "WARNING 09-12 20:07:56 [rocm.py:338] Model architecture 'Qwen2ForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 09-12 20:07:56 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-12 20:07:56 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-12 20:08:10 [weight_utils.py:308] Time spent downloading weights for llava-hf/llava-onevision-qwen2-7b-ov-hf: 13.937792 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:05,  1.72s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:01,  1.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.29s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.46s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:08:16 [default_loader.py:272] Loading weights took 5.88 seconds\n",
      "INFO 09-12 20:08:16 [gpu_model_runner.py:1782] Model loading took 15.1484 GiB and 20.123120 seconds\n",
      "INFO 09-12 20:08:17 [gpu_model_runner.py:2221] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 6 video items of the maximum feature size.\n",
      "INFO 09-12 20:08:34 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/8ec670dcc6/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-12 20:08:34 [backends.py:520] Dynamo bytecode transform time: 4.07 s\n",
      "INFO 09-12 20:08:49 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 09-12 20:08:49 [backends.py:193] Compiling a graph for general shape takes 12.85 s\n",
      "INFO 09-12 20:08:51 [monitor.py:34] torch.compile takes 16.92 s in total\n",
      "INFO 09-12 20:08:53 [gpu_worker.py:232] Available KV cache memory: 150.55 GiB\n",
      "INFO 09-12 20:08:53 [kv_cache_utils.py:716] GPU KV cache size: 2,819,040 tokens\n",
      "INFO 09-12 20:08:53 [kv_cache_utils.py:720] Maximum concurrency for 16,384 tokens per request: 172.06x\n",
      "INFO 09-12 20:08:53 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:13<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-12 20:09:06 [gpu_model_runner.py:2306] Graph capturing finished in 13 secs, took 0.27 GiB\n",
      "INFO 09-12 20:09:06 [core.py:172] init engine (profile, create kv cache, warmup model) took 49.89 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4207af6efed4419ad80f4b6cfc597ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69155e39114e4e18a35858debb1dfaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-- generate time = 1.961785078048706\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "The video is humorous because it shows a baby with a disproportionately large head compared to its body, wearing oversized glasses, and engaging in a pretend reading session.\n",
      "--------------------------------------------------\n",
      "The video is humorous because it shows a child with a blue face and glasses pretending to read a book, which is an amusing and unexpected behavior for a child.\n",
      "--------------------------------------------------\n",
      "The video is funny because the child's exaggerated actions and expressions, such as the large glasses and the dramatic reading, create a humorous and endearing scene.\n",
      "--------------------------------------------------\n",
      "The video is funny because the child's actions are exaggerated and humorous, such as waving their hands and adjusting the glasses.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W912 20:09:10.379902257 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "args = Args(\n",
    "    model_type=\"llava-onevision\",\n",
    "    num_prompts=4,\n",
    "    modality=\"video\",\n",
    "    num_frames=16,\n",
    "    seed=None,\n",
    "    image_repeat_prob=None,\n",
    "    disable_mm_processor_cache=False,\n",
    "    time_generate=True,\n",
    "    use_different_prompt_per_request=False\n",
    ")\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45fc44-f91f-4660-9173-799bb789a056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
