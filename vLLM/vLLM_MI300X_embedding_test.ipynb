{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557acbbb-073f-45bb-979e-b344a204d284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:39:22 [__init__.py:244] Automatically detected platform rocm.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eeb563a-13a2-47af-94a5-60fa8b1857a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28172ea47ea640c49ca022b534566985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3562399881b4a5ca88a4c596d9d6ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/55.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:42:46 [config.py:588] Found sentence-transformers tokenize configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b761def2779462985cdb5ab4855a6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:43:02 [config.py:484] Found sentence-transformers modules configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4486c1f84f1403b9cb59745fa420022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:43:02 [config.py:504] Found pooling configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5443da4cc8a44fa9845b4216d121a452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/981 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:43:02 [config.py:1467] Using max model len 32768\n",
      "INFO 08-08 15:43:02 [arg_utils.py:1580] (Enabling) chunked prefill by default\n",
      "INFO 08-08 15:43:02 [arg_utils.py:1583] (Enabling) prefix caching by default\n",
      "INFO 08-08 15:43:10 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 08-08 15:43:10 [rocm.py:288] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-08 15:43:10 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542573f439094548aa507a526f977aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb13d67cdb0246cfa22dc50ebb78e8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2c6b1863b84dadb947eabb06369004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815d66b889b14dc8a9517b5d90caf6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-08 15:43:11 [utils.py:2613] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 08-08 15:43:14 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 08-08 15:43:23 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 08-08 15:43:23 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='intfloat/e5-mistral-7b-instruct', speculative_config=None, tokenizer='intfloat/e5-mistral-7b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=intfloat/e5-mistral-7b-instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type='LAST', normalize=True, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "WARNING 08-08 15:43:23 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7d9a174bf320>\n",
      "INFO 08-08 15:43:23 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-08 15:43:24 [gpu_model_runner.py:1751] Starting to load model intfloat/e5-mistral-7b-instruct...\n",
      "INFO 08-08 15:43:24 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 08-08 15:43:24 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 08-08 15:43:24 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 08-08 15:44:10 [weight_utils.py:308] Time spent downloading weights for intfloat/e5-mistral-7b-instruct: 46.452413 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.33s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.50s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-08 15:44:16 [default_loader.py:272] Loading weights took 5.11 seconds\n",
      "INFO 08-08 15:44:16 [gpu_model_runner.py:1782] Model loading took 13.5449 GiB and 51.957554 seconds\n",
      "INFO 08-08 15:44:52 [gpu_worker.py:232] Available KV cache memory: 156.58 GiB\n",
      "INFO 08-08 15:44:52 [kv_cache_utils.py:716] GPU KV cache size: 1,282,688 tokens\n",
      "INFO 08-08 15:44:52 [kv_cache_utils.py:720] Maximum concurrency for 32,768 tokens per request: 62.58x\n",
      "INFO 08-08 15:44:52 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 08-08 15:44:52 [core.py:172] init engine (profile, create kv cache, warmup model) took 36.03 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\n",
    "    model=\"intfloat/e5-mistral-7b-instruct\",\n",
    "    task=\"embed\",\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563e3f5c-e271-44df-8ec7-bc3fc8a4322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d55ff44-c55c-4d70-98d7-f69034fa31d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d72ea0a210f4a1e8acdc1f74d67751c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6aec81eed9464089b24b02f1f91c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = model.embed(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a580124-79eb-4091-bc87-ba70c73e0d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Hello, my name is' | Embeddings: [0.009846453554928303, 0.024547548964619637, -0.017784448340535164, -0.0018040898721665144, 0.010418991558253765, -0.018631327897310257, -0.015470443293452263, 0.016019124537706375, 0.009100962430238724, 0.0014507267624139786, 0.02083798311650753, -0.007949923165142536, 0.01555393822491169, 0.0028686518780887127, 0.004613102413713932, 0.0038288452196866274, ...] (size=4096)\n",
      "Prompt: 'The president of the United States is' | Embeddings: [-0.0015292427269741893, -0.005219066981226206, -0.02025565691292286, 0.00983733031898737, 0.0006437179399654269, -0.028468837961554527, -0.0015308932634070516, 0.01411557849496603, 0.02582794427871704, -0.001001889118924737, 0.019727477803826332, -0.0018981426255777478, 0.022949369624257088, 0.001009316649287939, 0.010629598051309586, -0.010167442262172699, ...] (size=4096)\n",
      "Prompt: 'The capital of France is' | Embeddings: [0.013343879021704197, 0.007161386776715517, -0.014464455656707287, 0.002740260912105441, 0.016911692917346954, -0.016113121062517166, -0.006890902761369944, 0.009962828829884529, 0.037404078990221024, 0.0031411568634212017, 0.0032232680823653936, -0.008243323303759098, 0.008880892768502235, 0.008520247414708138, -0.002540617948397994, 0.0008050120086409152, ...] (size=4096)\n",
      "Prompt: 'The future of AI is' | Embeddings: [0.0023656946141272783, -0.013219870626926422, 0.004422543570399284, 0.004635869991034269, 0.005228089634329081, 0.0038780835457146168, -0.005508279427886009, 0.010175989009439945, 0.03232372924685478, 0.009609241038560867, -0.00039978805580176413, -0.00736135384067893, 0.01390760950744152, 0.005807573441416025, 0.009927638806402683, -0.003422774840146303, ...] (size=4096)\n"
     ]
    }
   ],
   "source": [
    "for prompt, output in zip(prompts, outputs):\n",
    "    embeds = output.outputs.embedding\n",
    "    embeds_trimmed = ((str(embeds[:16])[:-1] +\n",
    "                       \", ...]\") if len(embeds) > 16 else embeds)\n",
    "    print(f\"Prompt: {prompt!r} | \"\n",
    "          f\"Embeddings: {embeds_trimmed} (size={len(embeds)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c33252a-a100-42b8-b3b9-bd7583a755b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vLLM Blog\n",
    "long_prompt = \"\"\"vLLM Now Supports gpt-oss\n",
    "Aug 5, 2025 • The vLLM Team\n",
    "\n",
    "We’re thrilled to announce that vLLM now supports gpt-oss on NVIDIA Blackwell and Hopper GPUs, as well as AMD MI300x and MI355x GPUs. In this blog post, we’ll explore the efficient model architecture of gpt-oss and how vLLM supports it.\n",
    "\n",
    "To quickly get started with gpt-oss, you try our container:\n",
    "\n",
    "docker run --gpus all \\\n",
    "    -p 8000:8000 \\\n",
    "    --ipc=host \\\n",
    "    vllm/vllm-openai:gptoss \\\n",
    "    --model openai/gpt-oss-20b\n",
    "or install it in your virtual environment\n",
    "\n",
    "uv pip install --pre vllm==0.10.1+gptoss \\\n",
    "    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n",
    "    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n",
    "    --index-strategy unsafe-best-match\n",
    "\n",
    "vllm serve openai/gpt-oss-120b\n",
    "See vLLM User Guide for more detail.\n",
    "\n",
    "MXFP4 MoE\n",
    "gpt-oss is a sparse MoE model with 128 experts (120B) or 32 experts (20B), where each token is routed to 4 experts (with no shared expert). For the MoE weights, it uses MXFP4, a novel group-quantized floating-point format, while it uses the standard bfloat16 for attention and other layers. Since MoE takes the majority of the model parameters, using MXFP4 for MoE weights alone reduces the model sizes to 63 GB (120B) and 14 GB (20B), making them runnable on a single GPU (while often not recommended for the best performance)!\n",
    "\n",
    "In MXFP4, each weight is represented as a 4-bit floating-point (fp4 e2m1). Additionally, MXFP4 introduces a power-of-two scaling factor for each group of 32 consecutive fp4 values, to represent a wide numerical range. When it runs on hardware, two fp4 values are packed into a single 8-bit unit in memory, and then unpacked on the fly within the matmul kernel for computation.\n",
    "\n",
    "To efficiently run MXFP4 MoE, vLLM has integrated two specialized GPU kernels via collaboration with OpenAI and NVIDIA:\n",
    "\n",
    "Blackwell GPUs (e.g., B200): A new MoE kernel from FlashInfer. This kernel is implemented by NVIDIA and uses Blackwell’s native MXFP4 tensor cores for maximum performance.\n",
    "Hopper GPUs (e.g., H100, H200): Triton matmul_ogs kernel, officially implemented by the OpenAI Triton team. This kernel is optimized specifically for Hopper architectures, includes the swizzling optimization and built-in heuristics, removing the need for manual tuning.\n",
    "Efficient Attention\n",
    "gpt-oss has a highly efficient attention design. It uses GQA with 64 query heads and 8 KV heads. Importantly, the model interleaves full attention and sliding window attention (with window size 128) with 1:1 ratio. Furthermore, the head size of the model is 64, 50% of the standard head size 128. Finally, each query head has a trained “attention sink” vector.\n",
    "\n",
    "To efficiently support this attention, vLLM has integrated special GPU kernels from FlashInfer (Blackwell) and FlashAttention 3 (Hopper). Also, we enhanced our Triton attention kernel to support this on AMD GPUs.\n",
    "\n",
    "Furthermore, to efficiently manage the KV cache with different types of attention (i.e., full and sliding window), vLLM has integrated the hybrid KV cache allocator, a novel technique proposed by the vLLM team. With the hybrid KV cache manager, vLLM can dynamically share the KV cache space between the full attention layers and sliding window attention layers, reducing the potential memory fragmentation down to zero.\n",
    "\n",
    "Built-in Tool Support: Agent Loop & Tool Server via MCP\n",
    "gpt-oss includes built-in support for powerful tools, such as web browsing and Python code interpreter. When enabled, the model autonomously decides when and how to invoke these tools, interpreting the results seamlessly.\n",
    "\n",
    "vLLM natively supports these capabilities by integrating the OpenAI Responses API and the gpt-oss toolkit. Through this integration, vLLM implements a loop to parse the model’s tool call, actually invoke the search and code interpreter tools, parse their outputs, and send them back to the model.\n",
    "\n",
    "Alternatively, users can launch an MCP-compliant external tool server, to let vLLM use the tool server instead of directly leveraging the gpt-oss toolkit. This modular architecture simplifies the creation of scalable tool-calling libraries and services, requiring no internal changes to vLLM.\n",
    "\n",
    "Looking Ahead\n",
    "This announcement is just the beginning of vLLM’s continued optimization for gpt-oss. Our ongoing roadmap includes:\n",
    "\n",
    "Hardening the Responses API\n",
    "\n",
    "Further enhancing attention DP and MoE EP support\n",
    "\n",
    "Reducing CPU overhead to maximize throughput\n",
    "\n",
    "Acknowledgement\n",
    "vLLM team members who contributed to this effort are: Yongye Zhu, Woosuk Kwon, Chen Zhang, Simon Mo, Kaichao You.\n",
    "\n",
    "Jay Shah from Colfax International implemented the necessary changes to adapt to attention sinks and uncovered optimizations in the FA3 algorithm for gpt-oss.\n",
    "\n",
    "We want to thank OpenAI for the amazing partnership: Zhuohan Li, Xiaoxuan Liu, Philippe Tillet, Mario Lezcano-Casado, Dominik Kundel, Casey Dvorak, Vol Kyrylov.\n",
    "\n",
    "NVIDIA and vLLM worked closely to develop and verify both performance and accuracy on NVIDIA Blackwell architecture: Duncan Moss, Grace Ho, Julien Demouth, Minseok Lee, Siyuan Fu, Zihao Ye, Pen Chung Li.\n",
    "\n",
    "The AMD team contributed significantly to the integration of the model on their devices: Hongxia Yang, Ali Zaidy, with great support from Peng Sun, Vinayak Gokhale, Andy Luo\n",
    "\n",
    "The Hugging Face team continues to be amazing at building an open source ecosystem: Lysandre, Hugo, Marc, vb, Arthur, Mohamed, Andrien.\n",
    "\n",
    "Finally, we want to thank all the partners that leveraged vLLM in some way and delivered valuable feedback and improvements to this effort: AWS, Cloudflare, Snowflake, Databricks, Together, Fireworks, Cerebras.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bec437-64e8-4594-ad41-d799b9d00591",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prompt_output = model.embed(long_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b66355af-adf4-4873-801b-a2c49ff873e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: [0.020381629467010498, -0.008950795978307724, 0.013079996220767498, -0.00748417479917407, -0.0022219629026949406, -0.009863500483334064, 0.010474068112671375, 0.03064168430864811, 0.017712756991386414, 0.019261207431554794, 0.0047429148107767105, -0.005728005897253752, 7.632096094312146e-05, 0.005526581779122353, -0.010241171345114708, -0.007490469608455896, ...] (size=4096)\n"
     ]
    }
   ],
   "source": [
    "for output in long_prompt_output:\n",
    "    embeds = output.outputs.embedding\n",
    "    embeds_trimmed = ((str(embeds[:16])[:-1] +\n",
    "                       \", ...]\") if len(embeds) > 16 else embeds)\n",
    "    print(f\"Embeddings: {embeds_trimmed} (size={len(embeds)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b2e35-9cb3-495a-a94e-c2f8a4c93dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
