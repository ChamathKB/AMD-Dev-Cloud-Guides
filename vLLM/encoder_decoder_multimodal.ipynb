{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d73824f-c158-4676-9089-e20ed5411dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 20:06:38 [__init__.py:244] Automatically detected platform rocm.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections.abc import Sequence\n",
    "from dataclasses import asdict\n",
    "from typing import NamedTuple\n",
    "\n",
    "from vllm import LLM, EngineArgs, PromptType, SamplingParams\n",
    "from vllm.assets.audio import AudioAsset\n",
    "from vllm.assets.image import ImageAsset\n",
    "from vllm.utils import FlexibleArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958d188c-d049-4412-8faa-5a3b1d19c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRequestData(NamedTuple):\n",
    "    engine_args: EngineArgs\n",
    "    prompts: Sequence[PromptType]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d20087-0dec-444a-89df-b9d8d87f6460",
   "metadata": {},
   "source": [
    "#### Florence-2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6dfd17-8156-4b3c-8c85-a5c696a0ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_florence2():\n",
    "    engine_args = EngineArgs(\n",
    "        model=\"microsoft/Florence-2-large\",\n",
    "        tokenizer=\"Isotr0py/Florence-2-tokenizer\",\n",
    "        max_num_seqs=8,\n",
    "        trust_remote_code=True,\n",
    "        limit_mm_per_prompt={\"image\": 1},\n",
    "        dtype=\"half\",\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        {  # implicit prompt with task token\n",
    "            \"prompt\": \"<DETAILED_CAPTION>\",\n",
    "            \"multi_modal_data\": {\"image\": ImageAsset(\"stop_sign\").pil_image},\n",
    "        },\n",
    "        {  # explicit encoder/decoder prompt\n",
    "            \"encoder_prompt\": {\n",
    "                \"prompt\": \"Describe in detail what is shown in the image.\",\n",
    "                \"multi_modal_data\": {\"image\": ImageAsset(\"cherry_blossom\").pil_image},\n",
    "            },\n",
    "            \"decoder_prompt\": \"\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83249c34-6ee7-4a6c-a831-6d56a6c43a93",
   "metadata": {},
   "source": [
    "#### Llama-3.2-11B-Vision-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4ca122-cf0b-499d-a21b-cb56988ea44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mllama():\n",
    "    engine_args = EngineArgs(\n",
    "        model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "        max_model_len=8192,\n",
    "        max_num_seqs=2,\n",
    "        limit_mm_per_prompt={\"image\": 1},\n",
    "        dtype=\"half\",\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        {  # Implicit prompt\n",
    "            \"prompt\": \"<|image|><|begin_of_text|>What is the content of this image?\",  # noqa: E501\n",
    "            \"multi_modal_data\": {\n",
    "                \"image\": ImageAsset(\"stop_sign\").pil_image,\n",
    "            },\n",
    "        },\n",
    "        {  # Explicit prompt\n",
    "            \"encoder_prompt\": {\n",
    "                \"prompt\": \"<|image|>\",\n",
    "                \"multi_modal_data\": {\n",
    "                    \"image\": ImageAsset(\"stop_sign\").pil_image,\n",
    "                },\n",
    "            },\n",
    "            \"decoder_prompt\": \"<|image|><|begin_of_text|>Please describe the image.\",  # noqa: E501\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2622b-1504-4081-87ee-178d22fb7f5b",
   "metadata": {},
   "source": [
    "#### Whisper-large-v3-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2d02a0-88b1-450e-9fdb-4fe57a67623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_whisper():\n",
    "    engine_args = EngineArgs(\n",
    "        model=\"openai/whisper-large-v3-turbo\",\n",
    "        max_model_len=448,\n",
    "        max_num_seqs=16,\n",
    "        limit_mm_per_prompt={\"audio\": 1},\n",
    "        dtype=\"half\",\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        {  # Test implicit prompt\n",
    "            \"prompt\": \"<|startoftranscript|>\",\n",
    "            \"multi_modal_data\": {\n",
    "                \"audio\": AudioAsset(\"mary_had_lamb\").audio_and_sample_rate,\n",
    "            },\n",
    "        },\n",
    "        {  # Test explicit encoder/decoder prompt\n",
    "            \"encoder_prompt\": {\n",
    "                \"prompt\": \"\",\n",
    "                \"multi_modal_data\": {\n",
    "                    \"audio\": AudioAsset(\"winning_call\").audio_and_sample_rate,\n",
    "                },\n",
    "            },\n",
    "            \"decoder_prompt\": \"<|startoftranscript|>\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    return ModelRequestData(\n",
    "        engine_args=engine_args,\n",
    "        prompts=prompts,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc5818-7677-496e-980c-b337541fd658",
   "metadata": {},
   "source": [
    "### Model Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f08915cb-e4bf-43b2-8364-7e98503284fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_example_map = {\n",
    "    \"florence2\": run_florence2, # bug in vLLM v0.9.2\n",
    "    \"mllama\": run_mllama,\n",
    "    \"whisper\": run_whisper,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbbd2a6c-1b08-4b41-a626-6b076c6ce723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, seed=42):\n",
    "    if model not in model_example_map:\n",
    "        raise ValueError(f\"Model type {model} is not supported.\")\n",
    "\n",
    "    req_data = model_example_map[model]()\n",
    "\n",
    "    # Disable other modalities to save memory\n",
    "    default_limits = {\"image\": 0, \"video\": 0, \"audio\": 0}\n",
    "    req_data.engine_args.limit_mm_per_prompt = default_limits | dict(\n",
    "        req_data.engine_args.limit_mm_per_prompt or {}\n",
    "    )\n",
    "\n",
    "    engine_args = asdict(req_data.engine_args) | {\"seed\": seed}\n",
    "    llm = LLM(**engine_args)\n",
    "\n",
    "    prompts = req_data.prompts\n",
    "\n",
    "    # Create a sampling params object.\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=64,\n",
    "        skip_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Decoder prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "    duration = time.time() - start\n",
    "\n",
    "    print(\"Duration:\", duration)\n",
    "    print(\"RPS:\", len(prompts) / duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab98ee3f-1972-465b-ab32-f70aafbe050b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09b38e6468e404e86cf174e5a1db81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb36fed95404311b45ce7ea50633f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 20:07:16 [config.py:853] This model supports multiple tasks: {'score', 'reward', 'transcription', 'generate', 'classify', 'embed'}. Defaulting to 'transcription'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c68f517908c45cda2ae053336039082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 20:07:16 [config.py:1467] Using max model len 448\n",
      "WARNING 09-25 20:07:16 [config.py:978] CUDA graph is not supported for whisper on ROCm yet, fallback to eager mode.\n",
      "WARNING 09-25 20:07:16 [arg_utils.py:1719] ['WhisperForConditionalGeneration'] is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 09-25 20:07:25 [rocm.py:288] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 09-25 20:07:25 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "INFO 09-25 20:07:25 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='openai/whisper-large-v3-turbo', speculative_config=None, tokenizer='openai/whisper-large-v3-turbo', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=448, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=openai/whisper-large-v3-turbo, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ee04f5f1d44459937c124f76db7336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfc6c536bb2456ca8061c5d96c00850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff8fe8d010046039d417aa5d8d60fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42ca74b4c9945bbbec7706c9ab51c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0daba2a35d47c29b8c5ae81ab3100a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca7fc052720471dbd7ca1949923afb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f896f8916b8146d6a8a75841f8532911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 20:07:27 [rocm.py:233] Using ROCmFlashAttention backend.\n",
      "INFO 09-25 20:07:27 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-25 20:07:27 [model_runner.py:1171] Starting to load model openai/whisper-large-v3-turbo...\n",
      "INFO 09-25 20:07:27 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5749e67f2347cc9b357201f3659928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 20:07:29 [weight_utils.py:308] Time spent downloading weights for openai/whisper-large-v3-turbo: 1.537903 seconds\n",
      "INFO 09-25 20:07:29 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de902ac9ccd4a28948bbaa7b37c1587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 20:07:30 [default_loader.py:272] Loading weights took 0.68 seconds\n",
      "INFO 09-25 20:07:30 [model_runner.py:1203] Model loading took 1.9297 GiB and 2.599989 seconds\n",
      "INFO 09-25 20:07:31 [enc_dec_model_runner.py:315] Starting profile run for multi-modal models.\n",
      "WARNING 09-25 20:07:31 [registry.py:183] WhisperProcessor did not return `BatchFeature`. Make sure to match the behaviour of `ProcessorMixin` when implementing custom processors.\n",
      "INFO 09-25 20:07:59 [worker.py:294] Memory profiling takes 28.90 seconds\n",
      "INFO 09-25 20:07:59 [worker.py:294] the current vLLM instance can use total_gpu_memory (191.69GiB) x gpu_memory_utilization (0.90) = 172.52GiB\n",
      "INFO 09-25 20:07:59 [worker.py:294] model weights take 1.93GiB; non_torch_memory takes 0.68GiB; PyTorch activation peak memory takes 0.66GiB; the rest of the memory reserved for KV Cache is 169.25GiB.\n",
      "INFO 09-25 20:07:59 [executor_base.py:113] # rocm blocks: 69324, # CPU blocks: 1638\n",
      "INFO 09-25 20:07:59 [executor_base.py:118] Maximum concurrency for 448 tokens per request: 2475.86x\n",
      "INFO 09-25 20:08:00 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 29.92 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd2cf09ebaa4a8eb9f8148b2939125b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e762ef720a6649f09178b94d5bb7a0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder prompt: '<|startoftranscript|>', Generated text: '<|transcribe|><|notimestamps|> The first words I spoke in the original phonograph, a little piece of practical poetry. Mary had a little lamb, its streets were quite as slow, and everywhere that Mary went the lamb was sure to go.'\n",
      "Decoder prompt: '<|startoftranscript|>', Generated text: \"<|transcribe|><|notimestamps|> And the 0-1 pitch on the way to Edgar Martinez. Swung on the line down the left field line for a base hit. Here comes Joy. Here is Junior to third base. They're going to wave him in. The throw to the plate will be late. The Mariners are going to\"\n",
      "Duration: 2.00260329246521\n",
      "RPS: 0.9987000458478198\n"
     ]
    }
   ],
   "source": [
    "main(\"whisper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb861f35-2740-4dbb-abee-210cc7d77b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
