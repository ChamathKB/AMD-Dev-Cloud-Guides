{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c063f7f6-e8fd-4a47-8bae-2b0b210e330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "\n",
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92599a68-263a-43b7-82ac-4803c3d8b416",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a80425-400a-4e24-8647-d71ad8c72555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a31d89ea96f4a02babed13edf60c5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 14:49:04 [config.py:853] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2450d6168c9944548021a8481d7c06c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 14:49:05 [config.py:1467] Using max model len 40960\n",
      "INFO 09-05 14:49:12 [config.py:2267] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 09-05 14:49:12 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765b51dbc8fe4a93947ff3dbe26f56d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514881f121d243af8d2af228df987e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523ea09779af48fbbf6a6e2548cb9182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fa7d5f954349979911f8c2c149e1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-05 14:49:14 [utils.py:2613] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 09-05 14:49:17 [__init__.py:244] Automatically detected platform rocm.\n",
      "INFO 09-05 14:49:26 [core.py:459] Waiting for init message from front-end.\n",
      "INFO 09-05 14:49:26 [core.py:69] Initializing a V1 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='Qwen/Qwen3-30B-A3B', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 09-05 14:49:26 [utils.py:2753] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x738ad89b6f60>\n",
      "INFO 09-05 14:49:27 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-05 14:49:27 [gpu_model_runner.py:1751] Starting to load model Qwen/Qwen3-30B-A3B...\n",
      "INFO 09-05 14:49:27 [gpu_model_runner.py:1756] Loading model from scratch...\n",
      "INFO 09-05 14:49:27 [rocm.py:224] Using Triton Attention backend on V1 engine.\n",
      "INFO 09-05 14:49:27 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-05 14:50:15 [weight_utils.py:308] Time spent downloading weights for Qwen/Qwen3-30B-A3B: 48.174831 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:01<00:24,  1.62s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:03<00:24,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:05<00:22,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:07<00:21,  1.79s/it]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:07<00:15,  1.37s/it]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:09<00:14,  1.46s/it]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:11<00:14,  1.56s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:12<00:13,  1.64s/it]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:14<00:11,  1.68s/it]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:16<00:10,  1.71s/it]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:18<00:08,  1.74s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 12/16 [00:20<00:06,  1.75s/it]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 13/16 [00:21<00:05,  1.76s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 14/16 [00:23<00:03,  1.77s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 15/16 [00:25<00:01,  1.77s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:27<00:00,  1.76s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 16/16 [00:27<00:00,  1.70s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 14:50:43 [default_loader.py:272] Loading weights took 27.31 seconds\n",
      "INFO 09-05 14:50:43 [gpu_model_runner.py:1782] Model loading took 77.5273 GiB and 75.943223 seconds\n",
      "INFO 09-05 14:50:51 [backends.py:509] Using cache directory: /root/.cache/vllm/torch_compile_cache/fe72b23fcf/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 09-05 14:50:51 [backends.py:520] Dynamo bytecode transform time: 7.42 s\n",
      "INFO 09-05 14:51:23 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 09-05 14:51:23 [backends.py:193] Compiling a graph for general shape takes 28.65 s\n",
      "INFO 09-05 14:51:30 [monitor.py:34] torch.compile takes 36.08 s in total\n",
      "WARNING 09-05 14:51:43 [fused_moe.py:683] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=AMD_Instinct_MI300X.json\n",
      "INFO 09-05 14:51:45 [gpu_worker.py:232] Available KV cache memory: 88.58 GiB\n",
      "INFO 09-05 14:51:45 [kv_cache_utils.py:716] GPU KV cache size: 967,568 tokens\n",
      "INFO 09-05 14:51:45 [kv_cache_utils.py:720] Maximum concurrency for 40,960 tokens per request: 23.62x\n",
      "INFO 09-05 14:51:45 [rocm.py:224] Using Triton Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs: 100%|██████████| 67/67 [00:24<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 14:52:09 [gpu_model_runner.py:2306] Graph capturing finished in 24 secs, took 0.40 GiB\n",
      "INFO 09-05 14:52:10 [core.py:172] init engine (profile, create kv cache, warmup model) took 86.46 seconds\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-30B-A3B\"\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=8192,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30923416-575c-42d2-8cb0-d0e5dcff0b69",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8717c50-dbbc-4588-9bb0-2f0ce9dcb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_id(length=9):\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    random_id = \"\".join(random.choice(characters) for _ in range(length))\n",
    "    return random_id\n",
    "\n",
    "\n",
    "# simulate an API that can be called\n",
    "def get_current_weather(city: str, state: str, unit: \"str\"):\n",
    "    return (\n",
    "        f\"The weather in {city}, {state} is 85 degrees {unit}. It is \"\n",
    "        \"partly cloudly, with highs in the 90's.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a26e7d61-f744-4517-8fd0-f3e8eafca42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_functions = {\"get_current_weather\": get_current_weather}\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n",
    "                    },\n",
    "                    \"state\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"the two-letter abbreviation for the state that the city is\"\n",
    "                        \" in, e.g. 'CA' which would mean 'California'\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unit to fetch the temperature in\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"city\", \"state\", \"unit\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f35a8-17a0-4fd8-b52b-ff317b90633a",
   "metadata": {},
   "source": [
    "### Tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "832f73e2-3f8b-4793-9bbd-c6b1707eb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you tell me what the temperate will be in Dallas, in fahrenheit?\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ec847-4b72-4143-9eac-509a9c135077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4e0a2b78824c1cb8c0a2405c3a3633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349d389a9cb7462dbcefbdd25949d2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm.chat(\n",
    "    messages,\n",
    "    sampling_params=sampling_params,\n",
    "    tools=tools,\n",
    "    chat_template_kwargs={\"enable_thinking\": False},\n",
    ")\n",
    "\n",
    "output = outputs[0].outputs[0].text.strip()\n",
    "\n",
    "match = re.search(r'<tool_call>(.*?)</tool_call>', output, re.DOTALL)\n",
    "if match:\n",
    "    tool_output = match.group(1).strip()\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": tool_output,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d2b3dc03-8ba3-403b-a762-6674b4ff8956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"get_current_weather\", \"arguments\": {\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}}'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6ba9271-5a2c-4128-9623-a8d076cc7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = [json.loads(tool_output)]\n",
    "tool_answers = [\n",
    "    tool_functions[call[\"name\"]](**call[\"arguments\"]) for call in tool_calls\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "374f959c-483e-4f89-91ac-b2655b3b81fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46cdd1973314b8b8781960ecfc93e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3bc43550a2438491eb12550dff0bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in Dallas, TX is 85 degrees Fahrenheit. The weather is partly cloudy, with high temperatures expected in the 90s.\n"
     ]
    }
   ],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": \"\\n\\n\".join(tool_answers),\n",
    "        \"tool_call_id\": generate_random_id(),\n",
    "    }\n",
    ")\n",
    "\n",
    "outputs = llm.chat(messages, sampling_params, tools=tools, chat_template_kwargs={\"enable_thinking\": False})\n",
    "\n",
    "print(outputs[0].outputs[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b1299-d431-45e7-9bff-04959fe44700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
