{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0abfdd3-38c1-4ddf-9f1a-2da68435fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.benchmarks.datasets import add_dataset_parser, get_samples\n",
    "from vllm.v1.metrics.reader import Counter, Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4317366f-a3fa-4b26-a7f2-b4b13e7faa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd34f8c430b40258dfd7b8ce1327983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6bffbba-04d8-417c-b4a8-401677d412b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token validated successfully! Logged in as: Chamath\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d44cb6b7-670b-4405-ae30-9f285016173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"meta-llama/Llama-3.1-8B-Instruct\"  # The model to use\n",
    "output_len = 256                               # Maximum number of tokens to generate\n",
    "print_output = True                            # Whether to print the generated text\n",
    "\n",
    "# Speculative Decoding Configuration\n",
    "method = \"eagle\"                               # Choices: \"ngram\", \"eagle\", \"eagle3\"\n",
    "num_spec_tokens = 2                            # Number of speculative tokens\n",
    "EAGLE_DIR = None                               # Speculative model directory (leave None for default)\n",
    "# Ngram-specific (only used if method=\"ngram\")\n",
    "prompt_lookup_max = 5\n",
    "prompt_lookup_min = 2\n",
    "\n",
    "# vLLM/Hardware Configuration\n",
    "tp = 1                                         # Tensor parallelism size\n",
    "enforce_eager = False                          # Enforce eager execution\n",
    "enable_chunked_prefill = False                 # Enable chunked prefill\n",
    "\n",
    "# Sampling Parameters\n",
    "temp = 0.6                                     # Temperature (0 for deterministic)\n",
    "top_p = 1.0\n",
    "top_k = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b629bf6-47cf-4ebc-80be-b3bcaf953d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    \"\"\"Mock class for the expected prompt object.\"\"\"\n",
    "    def __init__(self, prompt_text):\n",
    "        self.prompt = prompt_text\n",
    "        \n",
    "def get_samples(args, tokenizer):\n",
    "    \"\"\"\n",
    "    Mock function to return a list of sample prompts.\n",
    "    In a real benchmark, this would load a dataset.\n",
    "    \"\"\"\n",
    "    print(\"Using manually defined prompts for notebook execution.\")\n",
    "    prompts = [\n",
    "        \"Explain the concept of quantum entanglement in simple terms.\",\n",
    "        \"Write a short, dramatic opening for a novel about a time traveler.\",\n",
    "        \"What are the three main types of rocks and how are they formed?\",\n",
    "    ]\n",
    "    return [Sample(p) for p in prompts]\n",
    "\n",
    "def add_dataset_parser(parser):\n",
    "    \"\"\"Mock function for dataset argument parsing.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac56c008-596d-49c8-a867-a2281d5b9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    endpoint_type = \"openai-chat\"\n",
    "\n",
    "    model_dir = MODEL_DIR\n",
    "    eagle_dir = EAGLE_DIR\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "    prompts = get_samples(args, tokenizer)\n",
    "    # add_special_tokens is False to avoid adding bos twice when using chat templates\n",
    "    prompt_ids = [\n",
    "        tokenizer.encode(prompt.prompt, add_special_tokens=False) for prompt in prompts\n",
    "    ]\n",
    "\n",
    "    if method == \"eagle\" or method == \"eagle3\":\n",
    "        if method == \"eagle\" and eagle_dir is None:\n",
    "            eagle_dir = \"yuhuili/EAGLE-LLaMA3.1-Instruct-8B\"\n",
    "\n",
    "        elif method == \"eagle3\" and eagle_dir is None:\n",
    "            eagle_dir = \"yuhuili/EAGLE3-LLaMA3.1-Instruct-8B\"\n",
    "        speculative_config = {\n",
    "            \"method\": method,\n",
    "            \"model\": eagle_dir,\n",
    "            \"num_speculative_tokens\": num_spec_tokens,\n",
    "        }\n",
    "    elif method == \"ngram\":\n",
    "        speculative_config = {\n",
    "            \"method\": \"ngram\",\n",
    "            \"num_speculative_tokens\": num_spec_tokens,\n",
    "            \"prompt_lookup_max\": prompt_lookup_max,\n",
    "            \"prompt_lookup_min\": prompt_lookup_min,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"unknown method: {method}\")\n",
    "\n",
    "    llm = LLM(\n",
    "        model=model_dir,\n",
    "        trust_remote_code=True,\n",
    "        tensor_parallel_size=tp,\n",
    "        enable_chunked_prefill=enable_chunked_prefill,\n",
    "        enforce_eager=enforce_eager,\n",
    "        gpu_memory_utilization=0.8,\n",
    "        speculative_config=speculative_config,\n",
    "        disable_log_stats=False,\n",
    "    )\n",
    "\n",
    "    sampling_params = SamplingParams(temperature=temp, max_tokens=output_len)\n",
    "    outputs = llm.generate(prompt_token_ids=prompt_ids, sampling_params=sampling_params)\n",
    "\n",
    "    # print the generated text\n",
    "    if print_output:\n",
    "        for output in outputs:\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"prompt: {output.prompt}\")\n",
    "            print(f\"generated text: {output.outputs[0].text}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        metrics = llm.get_metrics()\n",
    "    except AssertionError:\n",
    "        print(\"Metrics are not supported in the V0 engine.\")\n",
    "        return\n",
    "\n",
    "    total_num_output_tokens = sum(\n",
    "        len(output.outputs[0].token_ids) for output in outputs\n",
    "    )\n",
    "    num_drafts = 0\n",
    "    num_draft_tokens = 0\n",
    "    num_accepted_tokens = 0\n",
    "    acceptance_counts = [0] * num_spec_tokens\n",
    "    for metric in metrics:\n",
    "        if metric.name == \"vllm:spec_decode_num_drafts\":\n",
    "            assert isinstance(metric, Counter)\n",
    "            num_drafts += metric.value\n",
    "        elif metric.name == \"vllm:spec_decode_num_draft_tokens\":\n",
    "            assert isinstance(metric, Counter)\n",
    "            num_draft_tokens += metric.value\n",
    "        elif metric.name == \"vllm:spec_decode_num_accepted_tokens\":\n",
    "            assert isinstance(metric, Counter)\n",
    "            num_accepted_tokens += metric.value\n",
    "        elif metric.name == \"vllm:spec_decode_num_accepted_tokens_per_pos\":\n",
    "            assert isinstance(metric, Vector)\n",
    "            for pos in range(len(metric.values)):\n",
    "                acceptance_counts[pos] += metric.values[pos]\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"total_num_output_tokens: {total_num_output_tokens}\")\n",
    "    print(f\"num_drafts: {num_drafts}\")\n",
    "    print(f\"num_draft_tokens: {num_draft_tokens}\")\n",
    "    print(f\"num_accepted_tokens: {num_accepted_tokens}\")\n",
    "    acceptance_length = 1 + (num_accepted_tokens / num_drafts) if num_drafts > 0 else 1\n",
    "    print(f\"mean acceptance length: {acceptance_length:.2f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # print acceptance at each token position\n",
    "    for i in range(len(acceptance_counts)):\n",
    "        acceptance_rate = acceptance_counts[i] / num_drafts if num_drafts > 0 else 0\n",
    "        print(f\"acceptance at token {i}: {acceptance_rate:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247b932-4603-424c-94a3-e527f3768e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f90ecd-d367-43f5-96b3-7d853ad54a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b6cac-515e-4108-aba1-8b186de4087b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b83d2a-f1ac-48f6-b678-525ddd14e651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
