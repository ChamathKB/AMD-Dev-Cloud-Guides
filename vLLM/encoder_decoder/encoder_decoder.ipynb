{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329b8ea8-695a-4bb9-91c5-eeb1ecacdafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 19:46:25 [__init__.py:244] Automatically detected platform rocm.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.inputs import ExplicitEncoderDecoderPrompt, TextPrompt, TokensPrompt, zip_enc_dec_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be691a13-5c29-4f0a-91be-64b4f74e3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(tokenizer):\n",
    "    text_prompt_raw = \"Hello, my name is\"\n",
    "    text_prompt = TextPrompt(prompt=\"The president of the United States is\")\n",
    "    tokens_prompt = TokensPrompt(\n",
    "        prompt_token_ids=tokenizer.encode(prompt=\"The capital of France is\")\n",
    "    )\n",
    "\n",
    "    single_text_prompt_raw = text_prompt_raw  # Pass a string directly\n",
    "    single_text_prompt = text_prompt  # Pass a TextPrompt\n",
    "    single_tokens_prompt = tokens_prompt  # Pass a TokensPrompt\n",
    "\n",
    "    enc_dec_prompt1 = ExplicitEncoderDecoderPrompt(\n",
    "        # Pass encoder prompt string directly, &\n",
    "        # pass decoder prompt tokens\n",
    "        encoder_prompt=single_text_prompt_raw,\n",
    "        decoder_prompt=single_tokens_prompt,\n",
    "    )\n",
    "    enc_dec_prompt2 = ExplicitEncoderDecoderPrompt(\n",
    "        # Pass TextPrompt to encoder, and\n",
    "        # pass decoder prompt string directly\n",
    "        encoder_prompt=single_text_prompt,\n",
    "        decoder_prompt=single_text_prompt_raw,\n",
    "    )\n",
    "    enc_dec_prompt3 = ExplicitEncoderDecoderPrompt(\n",
    "        # Pass encoder prompt tokens directly, and\n",
    "        # pass TextPrompt to decoder\n",
    "        encoder_prompt=single_tokens_prompt,\n",
    "        decoder_prompt=single_text_prompt,\n",
    "    )\n",
    "\n",
    "    zipped_prompt_list = zip_enc_dec_prompts(\n",
    "        [\"An encoder prompt\", \"Another encoder prompt\"],\n",
    "        [\"A decoder prompt\", \"Another decoder prompt\"],\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        single_text_prompt_raw,\n",
    "        single_text_prompt,\n",
    "        single_tokens_prompt,\n",
    "        enc_dec_prompt1,\n",
    "        enc_dec_prompt2,\n",
    "        enc_dec_prompt3,\n",
    "    ] + zipped_prompt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f69b77-e1fb-49d6-bc08-14fbea0d0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sampling_params():\n",
    "    return SamplingParams(\n",
    "        temperature=0,\n",
    "        top_p=1.0,\n",
    "        min_tokens=0,\n",
    "        max_tokens=20,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a29ec7a-2e26-4f53-9484-c83f0f9c57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outputs(outputs):\n",
    "    print(\"-\" * 50)\n",
    "    for i, output in enumerate(outputs):\n",
    "        prompt = output.prompt\n",
    "        encoder_prompt = output.encoder_prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Output {i + 1}:\")\n",
    "        print(\n",
    "            f\"Encoder prompt: {encoder_prompt!r}\\n\"\n",
    "            f\"Decoder prompt: {prompt!r}\\n\"\n",
    "            f\"Generated text: {generated_text!r}\"\n",
    "        )\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b58ae50-ccc1-4e3f-85d6-dfce02f9bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dtype = \"float\"\n",
    "\n",
    "    llm = LLM(\n",
    "        model=\"facebook/bart-large-cnn\",\n",
    "        dtype=dtype,\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.llm_engine.get_tokenizer_group()\n",
    "\n",
    "    prompts = create_prompts(tokenizer)\n",
    "    sampling_params = create_sampling_params()\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    print_outputs(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406ca994-d7bc-47bc-acf3-5b3509e7e70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 19:46:50 [config.py:853] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 09-25 19:46:50 [config.py:1467] Using max model len 1024\n",
      "WARNING 09-25 19:46:50 [config.py:978] CUDA graph is not supported for bart on ROCm yet, fallback to eager mode.\n",
      "WARNING 09-25 19:46:50 [arg_utils.py:1719] --dtype torch.float32 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 09-25 19:46:57 [rocm.py:288] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 09-25 19:46:57 [config.py:4566] full_cuda_graph is not supported with cascade attention. Disabling cascade attention.\n",
      "INFO 09-25 19:46:57 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2.dev364+gb432b7a28) with config: model='facebook/bart-large-cnn', speculative_config=None, tokenizer='facebook/bart-large-cnn', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float32, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=facebook/bart-large-cnn, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"+rms_norm\",\"+silu_and_mul\"],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":true,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 09-25 19:46:58 [rocm.py:233] Using ROCmFlashAttention backend.\n",
      "INFO 09-25 19:46:58 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 09-25 19:46:58 [model_runner.py:1171] Starting to load model facebook/bart-large-cnn...\n",
      "INFO 09-25 19:46:59 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 09-25 19:46:59 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372a7cf852c84ded835d13bb2389a0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-25 19:46:59 [default_loader.py:272] Loading weights took 0.66 seconds\n",
      "INFO 09-25 19:46:59 [model_runner.py:1203] Model loading took 2.0527 GiB and 1.015113 seconds\n",
      "INFO 09-25 19:47:16 [worker.py:294] Memory profiling takes 15.94 seconds\n",
      "INFO 09-25 19:47:16 [worker.py:294] the current vLLM instance can use total_gpu_memory (191.69GiB) x gpu_memory_utilization (0.90) = 172.52GiB\n",
      "INFO 09-25 19:47:16 [worker.py:294] model weights take 2.05GiB; non_torch_memory takes 1.08GiB; PyTorch activation peak memory takes 0.61GiB; the rest of the memory reserved for KV Cache is 168.78GiB.\n",
      "INFO 09-25 19:47:16 [executor_base.py:113] # rocm blocks: 115218, # CPU blocks: 2730\n",
      "INFO 09-25 19:47:16 [executor_base.py:118] Maximum concurrency for 1024 tokens per request: 1800.28x\n",
      "INFO 09-25 19:47:17 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 17.19 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f188c8b2fced4061a95399bb7e969bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f0ded2c30749bba3a39144ff087ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Output 1:\n",
      "Encoder prompt: 'Hello, my name is'\n",
      "Decoder prompt: None\n",
      "Generated text: \"Hello, my name is John. I'm a writer.\"\n",
      "--------------------------------------------------\n",
      "Output 2:\n",
      "Encoder prompt: 'The president of the United States is'\n",
      "Decoder prompt: None\n",
      "Generated text: 'The president of the United States is.'\n",
      "--------------------------------------------------\n",
      "Output 3:\n",
      "Encoder prompt: None\n",
      "Decoder prompt: None\n",
      "Generated text: 'The capital of France is Paris.'\n",
      "--------------------------------------------------\n",
      "Output 4:\n",
      "Encoder prompt: 'Hello, my name is'\n",
      "Decoder prompt: None\n",
      "Generated text: 'is, a city in the south of France. The capital is the largest city in the country.'\n",
      "--------------------------------------------------\n",
      "Output 5:\n",
      "Encoder prompt: 'The president of the United States is'\n",
      "Decoder prompt: 'Hello, my name is'\n",
      "Generated text: \". I'm the president of the United States. I'm also the president of the United Nations.\"\n",
      "--------------------------------------------------\n",
      "Output 6:\n",
      "Encoder prompt: None\n",
      "Decoder prompt: 'The president of the United States is'\n",
      "Generated text: 'rence de France.'\n",
      "--------------------------------------------------\n",
      "Output 7:\n",
      "Encoder prompt: 'An encoder prompt'\n",
      "Decoder prompt: 'A decoder prompt'\n",
      "Generated text: 'i code prompt.'\n",
      "--------------------------------------------------\n",
      "Output 8:\n",
      "Encoder prompt: 'Another encoder prompt'\n",
      "Decoder prompt: 'Another decoder prompt'\n",
      "Generated text: 'i prompt.'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e19056-6df1-418d-91f0-7105bfc6af8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
