{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9043d1-4345-48a4-ae66-0c4b74828442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 19:18:22 [__init__.py:248] Automatically detected platform rocm.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fc911-2d26-4473-a5de-440b59d5d882",
   "metadata": {},
   "source": [
    "#### Set sampling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd545f49-beea-4b5b-873e-0bce625fb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    max_tokens=128,\n",
    "    skip_special_tokens=True,\n",
    "    temperature=0.8,\n",
    "    top_k=20,\n",
    "    top_p=0.95,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c8669-91ac-4379-a05c-3e8fe97c520d",
   "metadata": {},
   "source": [
    "#### Load Qwen2-7B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec5f5c57-7f5f-41f0-80b6-da96398c3752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 19:20:00 [__init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 07-10 19:20:00 [__init__.py:32] name=lora_filesystem_resolver, value=vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 07-10 19:20:00 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 07-10 19:20:00 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "INFO 07-10 19:20:00 [__init__.py:44] plugin lora_filesystem_resolver loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67026134108424cb3052c1e33452b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 19:20:11 [config.py:788] This model supports multiple tasks: {'score', 'embed', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-10 19:20:11 [arg_utils.py:1601] rocm is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.\n",
      "INFO 07-10 19:20:14 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.6.dev315+g91a560098) with config: model='Qwen/Qwen2-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd85f8dd6464be2811753f667b3d9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2b84da5ed24c638e03e5207c2730db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5106458ffdf14b2681148de491110771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dbd9ba98f642488ba03424510d54bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d184defac346bba4b744f545976e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 19:20:15 [rocm.py:192] None is not supported in AMD GPUs.\n",
      "INFO 07-10 19:20:15 [rocm.py:193] Using ROCmFlashAttention backend.\n",
      "INFO 07-10 19:20:15 [parallel_state.py:1079] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-10 19:20:15 [model_runner.py:1173] Starting to load model Qwen/Qwen2-7B-Instruct...\n",
      "WARNING 07-10 19:20:15 [rocm.py:298] Model architecture 'Qwen2ForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 07-10 19:20:16 [weight_utils.py:291] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44759d0096dc4fd98ad9e6d173c64f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fb5706aa554e2db664798bb514c2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1498b5e1de94761b2e97e0cc2705ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7c1ea9302d44ebb571ab56bb192a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 19:20:28 [weight_utils.py:307] Time spent downloading weights for Qwen/Qwen2-7B-Instruct: 12.399500 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd49a476f4e4478bb278f00fdde571b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60f5b01ee1c4cd5a451ceaca43c7537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 19:20:34 [default_loader.py:279] Loading weights took 5.43 seconds\n",
      "INFO 07-10 19:20:34 [model_runner.py:1205] Model loading took 14.5020 GiB and 18.356351 seconds\n",
      "INFO 07-10 19:21:03 [worker.py:320] Memory profiling takes 28.80 seconds\n",
      "INFO 07-10 19:21:03 [worker.py:320] the current vLLM instance can use total_gpu_memory (191.69GiB) x gpu_memory_utilization (0.90) = 172.52GiB\n",
      "INFO 07-10 19:21:03 [worker.py:320] model weights take 14.50GiB; non_torch_memory takes 0.56GiB; PyTorch activation peak memory takes 4.54GiB; the rest of the memory reserved for KV Cache is 152.92GiB.\n",
      "INFO 07-10 19:21:03 [executor_base.py:112] # rocm blocks: 178955, # CPU blocks: 4681\n",
      "INFO 07-10 19:21:03 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 87.38x\n",
      "INFO 07-10 19:21:04 [model_runner.py:1515] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32803452f81f474d906e92f64ffdfa49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 19:21:18 [model_runner.py:1673] Graph capturing finished in 14 secs, took 0.34 GiB\n",
      "INFO 07-10 19:21:18 [llm_engine.py:438] init engine (profile, create kv cache, warmup model) took 44.66 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"Qwen/Qwen2-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7567c4-94ff-4b58-8868-8574678d926f",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab45fda0-eb61-4ce2-b5e7-7163a16d292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Write a haiku about machine learning\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29cfc32d-4dc7-46e4-b8f6-3644d3a0f511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872c7a52e13c4d479c4756705a458c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e49d39089e043bea751609cd476a06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a haiku about machine learning: . Data flows in streams,\n",
      "Algorithms sift and learn,\n",
      "Predictions emerge.\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "prompt = prompts[0]\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(prompt + ': ' + generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016ddb0-90ed-4d2e-876f-109c3020f486",
   "metadata": {},
   "source": [
    "#### Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e87f55e-6e19-4434-a8d8-92de543d5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Who are the founders of AMD?\",\n",
    "    \"Where is AMD headquarters located?\",\n",
    "    \"How AMD invented CPU Zen Architecture?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63364ac1-a7a7-4b3e-982e-e9a4c2733e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    max_tokens=512,\n",
    "    skip_special_tokens=True,\n",
    "    temperature=0.8,\n",
    "    top_k=20,\n",
    "    top_p=0.95,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b25849-ed6f-4c8a-ba95-ab5ea873ef71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ae14e3ec5d432e8910f867738a9106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5eb5915bcd7402abcfcdb6d00909175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0% 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################################\n",
      "Who are the founders of AMD?:  The founders of AMD (Advanced Micro Devices) are Jerry Sanders and Jim Clark. Jerry Sanders founded AMD in 1969 as AMD Microelectronics, and Jim Clark joined the company in 1972. Sanders was the CEO and chairman of the company until his retirement in 2008, while Clark left the company in 1974 to pursue other ventures. In 1980, AMD was spun off from its parent company, Fairchild Semiconductor, and became an independent company.\n",
      "It's worth noting that AMD has had several other notable founders and leaders over the years, including David Owen (who helped develop the company's first microprocessors) and Lisa Su (who is currently the CEO of AMD). However, Jerry Sanders and Jim Clark are often cited as the original founders of the company.\n",
      "###################################################\n",
      "Where is AMD headquarters located?:  AMD (Advanced Micro Devices) is a company that designs and manufactures semiconductors, including microprocessors, embedded processors, and graphic processing units (GPUs). Its headquarters are located in Sunnyvale, California, United States. AMD was founded in 1969 and has a global presence with offices and facilities in various locations around the world.\n",
      "AMD's Sunnyvale headquarters serves as the company's primary location for research and development, manufacturing operations, and corporate functions. The campus is situated in the heart of Silicon Valley, a region known for its concentration of technology companies and innovation. Sunnyvale is a city in Santa Clara County, California, and is strategically located for AMD's operations within the tech industry.\n",
      "In addition to its Sunnyvale headquarters, AMD has other locations and facilities around the world. These include research and development centers, manufacturing plants, sales offices, and distribution centers. The company has a global presence, with offices in various countries such as China, Taiwan, Singapore, Japan, South Korea, Germany, France, and others. AMD's diverse geographic footprint enables the company to serve customers and partners globally, supporting its growth and expanding its market reach.\n",
      "AMD's headquarters in Sunnyvale, California, plays a crucial role in the company's operations and serves as the hub for its core business activities. The location in Silicon Valley provides AMD with access to a vibrant ecosystem of technology, innovation, and talent, contributing to the company's success and competitiveness in the semiconductor industry.\n",
      "###################################################\n",
      "How AMD invented CPU Zen Architecture?:  As an AI language model, I can provide you with an overview of how AMD developed the Zen CPU architecture.\n",
      "\n",
      "AMD began work on the Zen CPU architecture in 2012, with the goal of creating a new microarchitecture that would allow them to compete more effectively with Intel's high-end processors. The Zen design focused on improving the efficiency and performance of the CPU by making several key changes to the design.\n",
      "\n",
      "One of the key innovations of the Zen architecture was the introduction of a new instruction scheduler, which allowed the processor to execute more instructions per clock cycle. This was achieved through a number of optimizations, including the use of out-of-order execution and speculative execution.\n",
      "\n",
      "Another major change in the Zen design was the use of a new cache hierarchy. The Zen processors feature a large L1 cache, which allows them to store frequently used data closer to the CPU cores, reducing the time it takes to access that data. The L2 cache is also larger than in previous AMD processors, while the L3 cache has been moved into the CPU die for faster access.\n",
      "\n",
      "The Zen architecture also features several power-saving features, including dynamic clock frequency scaling and voltage adjustments. This allows the processor to adjust its power consumption and performance levels based on the workload it is performing.\n",
      "\n",
      "Overall, the Zen architecture represents a major departure from AMD's previous CPU designs and has proven to be a success, allowing AMD to compete more effectively with Intel in the high-end processor market. The architecture has been used in a range of AMD processors, including the Ryzen line of desktop and mobile processors, and the EPYC line of datacenter processors.\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    prompt = prompts[i]\n",
    "    generated_text = outputs[i].outputs[0].text\n",
    "    print(\"###################################################\")\n",
    "    print(prompt + ': ' + generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6265268-0229-40a1-8c39-5d67ce327ddd",
   "metadata": {},
   "source": [
    "**Release GPU vRAM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f09149e-e4dc-46c3-8549-bcecba865819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully released GPU memory!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "del llm\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Successfully released GPU memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34953eb-5093-44a4-9754-5ff684a67f35",
   "metadata": {},
   "source": [
    "#### Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f668f0d0-6043-4850-9bb1-48bd0bd70c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-10 19:41:29 [__init__.py:248] Automatically detected platform rocm.\n",
      "INFO 07-10 19:41:35 [__init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 07-10 19:41:35 [__init__.py:32] name=lora_filesystem_resolver, value=vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 07-10 19:41:35 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 07-10 19:41:35 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "INFO 07-10 19:41:35 [__init__.py:44] plugin lora_filesystem_resolver loaded.\n",
      "INFO 07-10 19:41:35 [api_server.py:1289] vLLM API server version 0.8.6.dev315+g91a560098\n",
      "INFO 07-10 19:41:36 [cli_args.py:300] non-default args: {}\n",
      "INFO 07-10 19:41:45 [config.py:788] This model supports multiple tasks: {'score', 'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-10 19:41:45 [api_server.py:257] Started engine process with PID 697\n",
      "INFO 07-10 19:41:48 [__init__.py:248] Automatically detected platform rocm.\n",
      "INFO 07-10 19:41:53 [__init__.py:30] Available plugins for group vllm.general_plugins:\n",
      "INFO 07-10 19:41:53 [__init__.py:32] name=lora_filesystem_resolver, value=vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 07-10 19:41:53 [__init__.py:34] all available plugins for group vllm.general_plugins will be loaded.\n",
      "INFO 07-10 19:41:53 [__init__.py:36] set environment variable VLLM_PLUGINS to control which plugins to load.\n",
      "INFO 07-10 19:41:53 [__init__.py:44] plugin lora_filesystem_resolver loaded.\n",
      "INFO 07-10 19:41:53 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.6.dev315+g91a560098) with config: model='Qwen/Qwen2-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=Qwen/Qwen2-7B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=True, \n",
      "INFO 07-10 19:41:54 [rocm.py:192] None is not supported in AMD GPUs.\n",
      "INFO 07-10 19:41:54 [rocm.py:193] Using ROCmFlashAttention backend.\n",
      "INFO 07-10 19:41:54 [parallel_state.py:1079] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-10 19:41:54 [model_runner.py:1173] Starting to load model Qwen/Qwen2-7B-Instruct...\n",
      "WARNING 07-10 19:41:54 [rocm.py:298] Model architecture 'Qwen2ForCausalLM' is partially supported by ROCm: Sliding window attention (SWA) is not yet supported in Triton flash attention. For half-precision SWA support, please use CK flash attention by setting `VLLM_USE_TRITON_FLASH_ATTN=0`\n",
      "INFO 07-10 19:41:54 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.32s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.40s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:04<00:01,  1.40s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.37s/it]\n",
      "\n",
      "INFO 07-10 19:42:00 [default_loader.py:279] Loading weights took 5.56 seconds\n",
      "INFO 07-10 19:42:00 [model_runner.py:1205] Model loading took 14.5020 GiB and 5.954995 seconds\n"
     ]
    }
   ],
   "source": [
    "! vllm serve Qwen/Qwen2-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17922ed-0455-4913-9d05-4f7f24b4f060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
